{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be30717-3db9-4e35-8b71-e15e8c110743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.41.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "0f55c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "3dadcb77-82c1-43e2-ae3d-1d7978c0cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "356488f8-bb87-45e5-a536-e43b972f010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "interactions = []\n",
    "\n",
    "for d in parse(\"/Users/zhiqiaogong/Projects/JupyterNotebook/cse258/hw3/train.json.gz\"):\n",
    "    u = d['userID']\n",
    "    i = d['gameID']\n",
    "    r = d['hours_transformed']\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    interactions.append((u,i,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "321d59bc-d6ae-4643-a614-ceda8be4259a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175000"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(interactions)\n",
    "len(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "38bf50b7-7a55-43e6-bc92-620d3668dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = int(len(interactions) * 0.9)\n",
    "nTest = len(interactions) - nTrain\n",
    "interactionsTrain = interactions[:nTrain]\n",
    "interactionsTest = interactions[nTrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "f1dde879-a150-40e3-9d7e-ccb4e15fc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "for u,i,r in interactionsTrain:\n",
    "    itemsPerUser[u].append(i)\n",
    "    usersPerItem[i].append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "6404cc3f-4ba1-4c91-9278-e57615d13104",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = sum([r for _,_,r in interactionsTrain]) / len(interactionsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "71c044a4-4eca-44d1-80b1-592983d052eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "K=1\n",
    "lamb_beta=0.00001\n",
    "lamb_gamma=0.0005\n",
    "learning_rate=0.1\n",
    "\n",
    "# optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate, decay = 0.001)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=50,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "class LatentFactorModel(tf.keras.Model):\n",
    "    def __init__(self, mu, K, lamb_beta, lamb_gamma):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = tf.Variable(mu)\n",
    "        # Initialize to small random values\n",
    "        # self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.001))\n",
    "        # self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        # Average rating per user\n",
    "        user_avg = {user: np.mean([r for _, u, r in interactions if u == user]) for user in userIDs}\n",
    "        # Average rating per item\n",
    "        item_avg = {item: np.mean([r for i, _, r in interactions if i == item]) for item in itemIDs}\n",
    "\n",
    "        # Initialize betaU and betaI with average ratings\n",
    "        self.betaU = tf.Variable([np.mean([r for _, u, r in interactions if u == user]) if user in userIDs else mu for user in range(len(userIDs))])\n",
    "        self.betaI = tf.Variable([np.mean([r for i, _, r in interactions if i == item]) if item in itemIDs else mu for item in range(len(itemIDs))])\n",
    "\n",
    "\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs), K], stddev=0.001))  # Smaller stddev\n",
    "        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs), K], stddev=0.001))\n",
    "        self.lamb_beta = lamb_beta\n",
    "        self.lamb_gamma = lamb_gamma\n",
    "\n",
    "    # Prediction for a single instance (useful for evaluation)\n",
    "    def predict(self, u, i):\n",
    "        p = self.alpha + self.betaU[u] + self.betaI[i] +\\\n",
    "            tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg_beta(self):\n",
    "        return self.lamb_beta * (tf.reduce_sum(self.betaU**2) + tf.reduce_sum(self.betaI**2))\n",
    "    def reg_gamma(self):\n",
    "        return self.lamb_gamma * (tf.reduce_sum(self.gammaU**2) + tf.reduce_sum(self.gammaI**2))\n",
    "\n",
    "    # def reg(self):\n",
    "    #     return self.lamb * (tf.reduce_sum(self.betaU**2) +\\\n",
    "    #                         tf.reduce_sum(self.betaI**2) +\\\n",
    "    #                         tf.reduce_sum(self.gammaU**2) +\\\n",
    "    #                         tf.reduce_sum(self.gammaI**2))\n",
    "    \n",
    "    # Prediction for a sample of instances\n",
    "    def predictSample(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        pred = self.alpha + beta_u + beta_i +\\\n",
    "               tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return pred\n",
    "    \n",
    "    # Loss\n",
    "    # def call(self, sampleU, sampleI, sampleR):\n",
    "    #     pred = self.predictSample(sampleU, sampleI)\n",
    "    #     r = tf.convert_to_tensor(sampleR, dtype=tf.float32)\n",
    "    #     return tf.nn.l2_loss(pred - r) / len(sampleR)\n",
    "    def call(self, sampleU, sampleI, sampleR):\n",
    "        pred = self.predictSample(sampleU, sampleI)\n",
    "        r = tf.convert_to_tensor(sampleR, dtype=tf.float32)\n",
    "        return tf.nn.l2_loss(pred - r) / len(sampleR) + self.reg_beta() + self.reg_gamma()\n",
    "\n",
    "###########\n",
    "modelLFM = LatentFactorModel(mu, K, lamb_beta, lamb_gamma)\n",
    "def trainingStep(model, train_data, val_data, optimizer):\n",
    "    def map_ids(data, userIDs, itemIDs):\n",
    "        return [(userIDs[u], itemIDs[i], r) for u, i, r in data]\n",
    "\n",
    "    # Map IDs for training data\n",
    "    train_data_mapped = map_ids(train_data, userIDs, itemIDs)\n",
    "    sampleU_train, sampleI_train, sampleR_train = zip(*train_data_mapped)\n",
    "    sampleU_train = tf.convert_to_tensor(sampleU_train, dtype=tf.int32)\n",
    "    sampleI_train = tf.convert_to_tensor(sampleI_train, dtype=tf.int32)\n",
    "    sampleR_train = tf.convert_to_tensor(sampleR_train, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        train_loss = model(sampleU_train, sampleI_train, sampleR_train)\n",
    "        # train_loss += model.reg()\n",
    "    # gradients = tape.gradient(train_loss, model.trainable_variables)\n",
    "    # optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    gradients = tape.gradient(train_loss, model.trainable_variables)\n",
    "    clipped_gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss = train_loss.numpy()\n",
    "\n",
    "    val_data_mapped = map_ids(val_data, userIDs, itemIDs)\n",
    "    sampleU_val, sampleI_val, sampleR_val = zip(*val_data_mapped)\n",
    "    sampleU_val = tf.convert_to_tensor(sampleU_val, dtype=tf.int32)\n",
    "    sampleI_val = tf.convert_to_tensor(sampleI_val, dtype=tf.int32)\n",
    "    sampleR_val = tf.convert_to_tensor(sampleR_val, dtype=tf.float32)\n",
    "\n",
    "    # val_loss = model(sampleU_val, sampleI_val, sampleR_val).numpy() + model.reg().numpy()\n",
    "    val_loss = model(sampleU_val, sampleI_val, sampleR_val)\n",
    "    \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "4c06da10-ce2c-463d-9f50-5f94093812fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "ca67c84b-12ad-412d-9846-36932c7ffbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 31.534032821655273, Validation Loss: 29.19029998779297\n",
      "Epoch 11, Training Loss: 13.331841468811035, Validation Loss: 11.989518165588379\n",
      "Epoch 21, Training Loss: 4.904242515563965, Validation Loss: 4.509769439697266\n",
      "Epoch 31, Training Loss: 2.559687614440918, Validation Loss: 2.5301759243011475\n",
      "Epoch 41, Training Loss: 2.214419364929199, Validation Loss: 2.2481439113616943\n",
      "Epoch 51, Training Loss: 1.7829275131225586, Validation Loss: 1.8511751890182495\n",
      "Epoch 61, Training Loss: 1.643142580986023, Validation Loss: 1.7466673851013184\n",
      "Epoch 71, Training Loss: 1.586799144744873, Validation Loss: 1.7022171020507812\n",
      "Epoch 81, Training Loss: 1.547845482826233, Validation Loss: 1.6771260499954224\n",
      "Epoch 91, Training Loss: 1.531005859375, Validation Loss: 1.6653110980987549\n",
      "Epoch 101, Training Loss: 1.517656922340393, Validation Loss: 1.6539571285247803\n",
      "Epoch 111, Training Loss: 1.5092713832855225, Validation Loss: 1.6467496156692505\n",
      "Epoch 121, Training Loss: 1.5024949312210083, Validation Loss: 1.641086459159851\n",
      "Epoch 131, Training Loss: 1.4970206022262573, Validation Loss: 1.6363024711608887\n",
      "Epoch 141, Training Loss: 1.4922510385513306, Validation Loss: 1.6318678855895996\n",
      "Epoch 151, Training Loss: 1.4880119562149048, Validation Loss: 1.6278971433639526\n",
      "Epoch 161, Training Loss: 1.4841585159301758, Validation Loss: 1.624309778213501\n",
      "Epoch 171, Training Loss: 1.4806196689605713, Validation Loss: 1.620976209640503\n",
      "Epoch 181, Training Loss: 1.4773470163345337, Validation Loss: 1.6178420782089233\n",
      "Epoch 191, Training Loss: 1.4743107557296753, Validation Loss: 1.6149147748947144\n",
      "Epoch 201, Training Loss: 1.4714877605438232, Validation Loss: 1.6121866703033447\n",
      "Epoch 211, Training Loss: 1.4688608646392822, Validation Loss: 1.6096374988555908\n",
      "Epoch 221, Training Loss: 1.4664157629013062, Validation Loss: 1.607254981994629\n",
      "Epoch 231, Training Loss: 1.4641395807266235, Validation Loss: 1.6050316095352173\n",
      "Epoch 241, Training Loss: 1.462020754814148, Validation Loss: 1.6029590368270874\n",
      "Epoch 251, Training Loss: 1.4600480794906616, Validation Loss: 1.6010265350341797\n",
      "Epoch 261, Training Loss: 1.4582116603851318, Validation Loss: 1.5992250442504883\n",
      "Epoch 271, Training Loss: 1.4565014839172363, Validation Loss: 1.5975459814071655\n",
      "Epoch 281, Training Loss: 1.4549087285995483, Validation Loss: 1.5959808826446533\n",
      "Epoch 291, Training Loss: 1.4534249305725098, Validation Loss: 1.5945218801498413\n",
      "Epoch 301, Training Loss: 1.4520416259765625, Validation Loss: 1.5931609869003296\n",
      "Epoch 311, Training Loss: 1.450751781463623, Validation Loss: 1.5918914079666138\n",
      "Epoch 321, Training Loss: 1.4495481252670288, Validation Loss: 1.5907061100006104\n",
      "Epoch 331, Training Loss: 1.4484243392944336, Validation Loss: 1.5895988941192627\n",
      "Epoch 341, Training Loss: 1.4473741054534912, Validation Loss: 1.5885640382766724\n",
      "Epoch 351, Training Loss: 1.4463919401168823, Validation Loss: 1.5875957012176514\n",
      "Epoch 361, Training Loss: 1.4454728364944458, Validation Loss: 1.5866891145706177\n",
      "Epoch 371, Training Loss: 1.4446120262145996, Validation Loss: 1.5858395099639893\n",
      "Epoch 381, Training Loss: 1.4438048601150513, Validation Loss: 1.5850427150726318\n",
      "Epoch 391, Training Loss: 1.4430474042892456, Validation Loss: 1.5842945575714111\n",
      "Epoch 401, Training Loss: 1.442336082458496, Validation Loss: 1.583591341972351\n",
      "Epoch 411, Training Loss: 1.4416674375534058, Validation Loss: 1.5829299688339233\n",
      "Epoch 421, Training Loss: 1.441037893295288, Validation Loss: 1.5823073387145996\n",
      "Epoch 431, Training Loss: 1.4404453039169312, Validation Loss: 1.5817205905914307\n",
      "Epoch 441, Training Loss: 1.4398863315582275, Validation Loss: 1.5811668634414673\n",
      "Epoch 451, Training Loss: 1.439359188079834, Validation Loss: 1.580643892288208\n",
      "Epoch 461, Training Loss: 1.4388611316680908, Validation Loss: 1.5801496505737305\n",
      "Epoch 471, Training Loss: 1.4383900165557861, Validation Loss: 1.5796821117401123\n",
      "Epoch 481, Training Loss: 1.4379444122314453, Validation Loss: 1.5792391300201416\n",
      "Epoch 491, Training Loss: 1.437522530555725, Validation Loss: 1.5788193941116333\n",
      "Epoch 501, Training Loss: 1.4371225833892822, Validation Loss: 1.5784212350845337\n",
      "Epoch 511, Training Loss: 1.4367430210113525, Validation Loss: 1.5780431032180786\n",
      "Epoch 521, Training Loss: 1.436382532119751, Validation Loss: 1.577683448791504\n",
      "Epoch 531, Training Loss: 1.4360401630401611, Validation Loss: 1.5773416757583618\n",
      "Epoch 541, Training Loss: 1.4357144832611084, Validation Loss: 1.5770162343978882\n",
      "Epoch 551, Training Loss: 1.4354045391082764, Validation Loss: 1.576706051826477\n",
      "Epoch 561, Training Loss: 1.4351091384887695, Validation Loss: 1.5764106512069702\n",
      "Epoch 571, Training Loss: 1.4348276853561401, Validation Loss: 1.5761284828186035\n",
      "Epoch 581, Training Loss: 1.4345592260360718, Validation Loss: 1.5758589506149292\n",
      "Epoch 591, Training Loss: 1.4343030452728271, Validation Loss: 1.5756014585494995\n",
      "Epoch 601, Training Loss: 1.4340583086013794, Validation Loss: 1.5753555297851562\n",
      "Epoch 611, Training Loss: 1.4338243007659912, Validation Loss: 1.575120210647583\n",
      "Epoch 621, Training Loss: 1.433600664138794, Validation Loss: 1.5748947858810425\n",
      "Epoch 631, Training Loss: 1.4333864450454712, Validation Loss: 1.5746787786483765\n",
      "Epoch 641, Training Loss: 1.4331815242767334, Validation Loss: 1.5744715929031372\n",
      "Epoch 651, Training Loss: 1.4329850673675537, Validation Loss: 1.5742732286453247\n",
      "Epoch 661, Training Loss: 1.432796835899353, Validation Loss: 1.5740827322006226\n",
      "Epoch 671, Training Loss: 1.4326163530349731, Validation Loss: 1.5738998651504517\n",
      "Epoch 681, Training Loss: 1.4324431419372559, Validation Loss: 1.5737241506576538\n",
      "Epoch 691, Training Loss: 1.432276725769043, Validation Loss: 1.5735551118850708\n",
      "Epoch 701, Training Loss: 1.4321168661117554, Validation Loss: 1.5733927488327026\n",
      "Epoch 711, Training Loss: 1.431963324546814, Validation Loss: 1.5732365846633911\n",
      "Epoch 721, Training Loss: 1.43181574344635, Validation Loss: 1.5730862617492676\n",
      "Epoch 731, Training Loss: 1.4316736459732056, Validation Loss: 1.572941541671753\n",
      "Epoch 741, Training Loss: 1.43153715133667, Validation Loss: 1.5728020668029785\n",
      "Epoch 751, Training Loss: 1.4314055442810059, Validation Loss: 1.5726675987243652\n",
      "Epoch 761, Training Loss: 1.4312788248062134, Validation Loss: 1.5725380182266235\n",
      "Epoch 771, Training Loss: 1.431156873703003, Validation Loss: 1.5724132061004639\n",
      "Epoch 781, Training Loss: 1.4310392141342163, Validation Loss: 1.5722925662994385\n",
      "Epoch 791, Training Loss: 1.430925726890564, Validation Loss: 1.5721760988235474\n",
      "Epoch 801, Training Loss: 1.4308162927627563, Validation Loss: 1.5720640420913696\n",
      "Epoch 811, Training Loss: 1.430710792541504, Validation Loss: 1.5719555616378784\n",
      "Epoch 821, Training Loss: 1.4306089878082275, Validation Loss: 1.5718507766723633\n",
      "Epoch 831, Training Loss: 1.4305105209350586, Validation Loss: 1.571749210357666\n",
      "Epoch 841, Training Loss: 1.4304156303405762, Validation Loss: 1.5716514587402344\n",
      "Epoch 851, Training Loss: 1.430323839187622, Validation Loss: 1.571556806564331\n",
      "Epoch 861, Training Loss: 1.4302352666854858, Validation Loss: 1.5714651346206665\n",
      "Epoch 871, Training Loss: 1.4301493167877197, Validation Loss: 1.5713764429092407\n",
      "Epoch 881, Training Loss: 1.430066466331482, Validation Loss: 1.5712906122207642\n",
      "Epoch 891, Training Loss: 1.4299863576889038, Validation Loss: 1.5712076425552368\n",
      "Epoch 901, Training Loss: 1.4299086332321167, Validation Loss: 1.571126937866211\n",
      "Epoch 911, Training Loss: 1.4298335313796997, Validation Loss: 1.5710489749908447\n",
      "Epoch 921, Training Loss: 1.4297608137130737, Validation Loss: 1.5709736347198486\n",
      "Epoch 931, Training Loss: 1.4296904802322388, Validation Loss: 1.5709004402160645\n",
      "Epoch 941, Training Loss: 1.4296224117279053, Validation Loss: 1.5708293914794922\n",
      "Epoch 951, Training Loss: 1.429556131362915, Validation Loss: 1.5707606077194214\n",
      "Epoch 961, Training Loss: 1.4294923543930054, Validation Loss: 1.5706939697265625\n",
      "Epoch 971, Training Loss: 1.4294302463531494, Validation Loss: 1.5706291198730469\n",
      "Epoch 981, Training Loss: 1.4293700456619263, Validation Loss: 1.5705665349960327\n",
      "Epoch 991, Training Loss: 1.4293118715286255, Validation Loss: 1.5705052614212036\n",
      "Epoch 1001, Training Loss: 1.4292551279067993, Validation Loss: 1.5704461336135864\n",
      "Epoch 1011, Training Loss: 1.429200291633606, Validation Loss: 1.5703887939453125\n",
      "Epoch 1021, Training Loss: 1.4291468858718872, Validation Loss: 1.5703328847885132\n",
      "Epoch 1031, Training Loss: 1.4290953874588013, Validation Loss: 1.570278525352478\n",
      "Epoch 1041, Training Loss: 1.4290449619293213, Validation Loss: 1.5702259540557861\n",
      "Epoch 1051, Training Loss: 1.428996205329895, Validation Loss: 1.5701745748519897\n",
      "Epoch 1061, Training Loss: 1.428948998451233, Validation Loss: 1.5701249837875366\n",
      "Epoch 1071, Training Loss: 1.4289029836654663, Validation Loss: 1.570076584815979\n",
      "Epoch 1081, Training Loss: 1.4288583993911743, Validation Loss: 1.5700294971466064\n",
      "Epoch 1091, Training Loss: 1.4288148880004883, Validation Loss: 1.5699836015701294\n",
      "Epoch 1101, Training Loss: 1.4287726879119873, Validation Loss: 1.5699390172958374\n",
      "Epoch 1111, Training Loss: 1.4287316799163818, Validation Loss: 1.56989586353302\n",
      "Epoch 1121, Training Loss: 1.4286917448043823, Validation Loss: 1.5698539018630981\n",
      "Epoch 1131, Training Loss: 1.4286530017852783, Validation Loss: 1.5698126554489136\n",
      "Epoch 1141, Training Loss: 1.4286150932312012, Validation Loss: 1.569772720336914\n",
      "Epoch 1151, Training Loss: 1.428578495979309, Validation Loss: 1.56973397731781\n",
      "Epoch 1161, Training Loss: 1.4285427331924438, Validation Loss: 1.5696961879730225\n",
      "Epoch 1171, Training Loss: 1.4285080432891846, Validation Loss: 1.5696593523025513\n",
      "Epoch 1181, Training Loss: 1.4284740686416626, Validation Loss: 1.5696234703063965\n",
      "Epoch 1191, Training Loss: 1.4284412860870361, Validation Loss: 1.569588303565979\n",
      "Epoch 1201, Training Loss: 1.428409218788147, Validation Loss: 1.569554328918457\n",
      "Epoch 1211, Training Loss: 1.4283778667449951, Validation Loss: 1.569521188735962\n",
      "Epoch 1221, Training Loss: 1.4283475875854492, Validation Loss: 1.569488763809204\n",
      "Epoch 1231, Training Loss: 1.428317904472351, Validation Loss: 1.5694571733474731\n",
      "Epoch 1241, Training Loss: 1.4282888174057007, Validation Loss: 1.5694262981414795\n",
      "Epoch 1251, Training Loss: 1.4282608032226562, Validation Loss: 1.5693966150283813\n",
      "Epoch 1261, Training Loss: 1.4282333850860596, Validation Loss: 1.5693671703338623\n",
      "Epoch 1271, Training Loss: 1.4282068014144897, Validation Loss: 1.5693385601043701\n",
      "Epoch 1281, Training Loss: 1.4281805753707886, Validation Loss: 1.5693109035491943\n",
      "Epoch 1291, Training Loss: 1.4281551837921143, Validation Loss: 1.5692836046218872\n",
      "Epoch 1301, Training Loss: 1.4281303882598877, Validation Loss: 1.5692569017410278\n",
      "Epoch 1311, Training Loss: 1.4281061887741089, Validation Loss: 1.569231390953064\n",
      "Epoch 1321, Training Loss: 1.4280827045440674, Validation Loss: 1.5692061185836792\n",
      "Epoch 1331, Training Loss: 1.428059458732605, Validation Loss: 1.5691814422607422\n",
      "Epoch 1341, Training Loss: 1.4280370473861694, Validation Loss: 1.5691574811935425\n",
      "Epoch 1351, Training Loss: 1.428015112876892, Validation Loss: 1.5691338777542114\n",
      "Epoch 1361, Training Loss: 1.4279937744140625, Validation Loss: 1.5691108703613281\n",
      "Epoch 1371, Training Loss: 1.4279729127883911, Validation Loss: 1.5690885782241821\n",
      "Epoch 1381, Training Loss: 1.427952527999878, Validation Loss: 1.5690666437149048\n",
      "Epoch 1391, Training Loss: 1.4279327392578125, Validation Loss: 1.5690453052520752\n",
      "Epoch 1401, Training Loss: 1.4279133081436157, Validation Loss: 1.569024682044983\n",
      "Epoch 1411, Training Loss: 1.4278943538665771, Validation Loss: 1.5690041780471802\n",
      "Epoch 1421, Training Loss: 1.4278758764266968, Validation Loss: 1.5689841508865356\n",
      "Epoch 1431, Training Loss: 1.4278576374053955, Validation Loss: 1.5689648389816284\n",
      "Epoch 1441, Training Loss: 1.4278401136398315, Validation Loss: 1.5689457654953003\n",
      "Epoch 1451, Training Loss: 1.4278228282928467, Validation Loss: 1.5689270496368408\n",
      "Epoch 1461, Training Loss: 1.42780601978302, Validation Loss: 1.568908929824829\n",
      "Epoch 1471, Training Loss: 1.427789568901062, Validation Loss: 1.5688912868499756\n",
      "Epoch 1481, Training Loss: 1.427773356437683, Validation Loss: 1.568873643875122\n",
      "Epoch 1491, Training Loss: 1.4277576208114624, Validation Loss: 1.5688568353652954\n",
      "Epoch 1501, Training Loss: 1.4277421236038208, Validation Loss: 1.5688400268554688\n",
      "Epoch 1511, Training Loss: 1.4277271032333374, Validation Loss: 1.5688238143920898\n",
      "Epoch 1521, Training Loss: 1.4277124404907227, Validation Loss: 1.5688079595565796\n",
      "Epoch 1531, Training Loss: 1.427698016166687, Validation Loss: 1.568792462348938\n",
      "Epoch 1541, Training Loss: 1.42768394947052, Validation Loss: 1.5687772035598755\n",
      "Epoch 1551, Training Loss: 1.4276701211929321, Validation Loss: 1.568762183189392\n",
      "Epoch 1561, Training Loss: 1.4276567697525024, Validation Loss: 1.5687475204467773\n",
      "Epoch 1571, Training Loss: 1.4276436567306519, Validation Loss: 1.5687334537506104\n",
      "Epoch 1581, Training Loss: 1.4276306629180908, Validation Loss: 1.568719506263733\n",
      "Epoch 1591, Training Loss: 1.427618145942688, Validation Loss: 1.5687057971954346\n",
      "Epoch 1601, Training Loss: 1.4276059865951538, Validation Loss: 1.5686923265457153\n",
      "Epoch 1611, Training Loss: 1.42759370803833, Validation Loss: 1.5686792135238647\n",
      "Epoch 1621, Training Loss: 1.427581787109375, Validation Loss: 1.5686664581298828\n",
      "Epoch 1631, Training Loss: 1.4275703430175781, Validation Loss: 1.5686538219451904\n",
      "Epoch 1641, Training Loss: 1.4275590181350708, Validation Loss: 1.5686417818069458\n",
      "Epoch 1651, Training Loss: 1.4275480508804321, Validation Loss: 1.5686296224594116\n",
      "Epoch 1661, Training Loss: 1.4275370836257935, Validation Loss: 1.568617820739746\n",
      "Epoch 1671, Training Loss: 1.4275264739990234, Validation Loss: 1.5686062574386597\n",
      "Epoch 1681, Training Loss: 1.427516222000122, Validation Loss: 1.5685951709747314\n",
      "Epoch 1691, Training Loss: 1.4275059700012207, Validation Loss: 1.5685839653015137\n",
      "Epoch 1701, Training Loss: 1.427496075630188, Validation Loss: 1.5685731172561646\n",
      "Epoch 1711, Training Loss: 1.4274863004684448, Validation Loss: 1.5685625076293945\n",
      "Epoch 1721, Training Loss: 1.4274767637252808, Validation Loss: 1.568552017211914\n",
      "Epoch 1731, Training Loss: 1.4274672269821167, Validation Loss: 1.5685418844223022\n",
      "Epoch 1741, Training Loss: 1.4274580478668213, Validation Loss: 1.56853187084198\n",
      "Epoch 1751, Training Loss: 1.4274492263793945, Validation Loss: 1.5685219764709473\n",
      "Epoch 1761, Training Loss: 1.4274404048919678, Validation Loss: 1.5685124397277832\n",
      "Epoch 1771, Training Loss: 1.4274317026138306, Validation Loss: 1.5685031414031982\n",
      "Epoch 1781, Training Loss: 1.427423357963562, Validation Loss: 1.5684938430786133\n",
      "Epoch 1791, Training Loss: 1.427415132522583, Validation Loss: 1.5684847831726074\n",
      "Epoch 1801, Training Loss: 1.427406907081604, Validation Loss: 1.5684759616851807\n",
      "Epoch 1811, Training Loss: 1.4273990392684937, Validation Loss: 1.5684672594070435\n",
      "Epoch 1821, Training Loss: 1.4273911714553833, Validation Loss: 1.5684586763381958\n",
      "Epoch 1831, Training Loss: 1.427383542060852, Validation Loss: 1.5684503316879272\n",
      "Epoch 1841, Training Loss: 1.4273760318756104, Validation Loss: 1.5684422254562378\n",
      "Epoch 1851, Training Loss: 1.4273686408996582, Validation Loss: 1.5684341192245483\n",
      "Epoch 1861, Training Loss: 1.4273614883422852, Validation Loss: 1.568426251411438\n",
      "Epoch 1871, Training Loss: 1.4273544549942017, Validation Loss: 1.5684185028076172\n",
      "Epoch 1881, Training Loss: 1.4273474216461182, Validation Loss: 1.568410873413086\n",
      "Epoch 1891, Training Loss: 1.4273408651351929, Validation Loss: 1.5684034824371338\n",
      "Epoch 1901, Training Loss: 1.4273340702056885, Validation Loss: 1.5683963298797607\n",
      "Epoch 1911, Training Loss: 1.4273275136947632, Validation Loss: 1.5683891773223877\n",
      "Epoch 1921, Training Loss: 1.4273213148117065, Validation Loss: 1.5683821439743042\n",
      "Epoch 1931, Training Loss: 1.4273148775100708, Validation Loss: 1.5683752298355103\n",
      "Epoch 1941, Training Loss: 1.4273089170455933, Validation Loss: 1.5683684349060059\n",
      "Epoch 1951, Training Loss: 1.4273027181625366, Validation Loss: 1.5683616399765015\n",
      "Epoch 1961, Training Loss: 1.4272969961166382, Validation Loss: 1.5683553218841553\n",
      "Epoch 1971, Training Loss: 1.427290916442871, Validation Loss: 1.568349003791809\n",
      "Epoch 1981, Training Loss: 1.4272853136062622, Validation Loss: 1.5683428049087524\n",
      "Epoch 1991, Training Loss: 1.4272797107696533, Validation Loss: 1.5683366060256958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelLFM = LatentFactorModel(mu, K, lamb_beta, lamb_gamma)\n",
    "\n",
    "\n",
    "train_data, val_data = train_test_split(interactionsTrain, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "\n",
    "# Variables to keep track of best loss and patience counter\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "pvalloss = float('inf')\n",
    "for epoch in range(2000):\n",
    "    train_loss, val_loss = trainingStep(modelLFM, train_data, val_data, optimizer)\n",
    "\n",
    "    if val_loss>pvalloss:\n",
    "        break\n",
    "    # # Early stopping logic\n",
    "    # if val_loss < best_val_loss:\n",
    "    #     best_val_loss = val_loss\n",
    "    #     patience_counter = 0\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    if epoch%10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # if patience_counter >= early_stopping.patience:\n",
    "    #     print(\"Early stopping triggered\")\n",
    "    #     break\n",
    "        \n",
    "    pvalloss = val_loss\n",
    "\n",
    "# batch_sizes = [100, 500, 1000]\n",
    "\n",
    "# for batch_size in batch_sizes:\n",
    "#     print(f\"Training with batch size: {batch_size}\")\n",
    "\n",
    "#     # Reset the model for each batch size\n",
    "#     modelLFM = LatentFactorModel(mu, K, lam)\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "#     best_val_loss = float('inf')\n",
    "#     patience_counter = 0\n",
    "\n",
    "#     for epoch in range(100):\n",
    "#         # Modify trainingStep to use a portion of data based on the current batch size\n",
    "#         train_loss, val_loss = trainingStep(modelLFM, train_data, val_data, userIDs, itemIDs, optimizer, batch_size)\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "#         if patience_counter >= early_stopping.patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#     # Evaluate the model's performance for the current batch size\n",
    "#     mse_test = calculate_mse(modelLFM, interactionsTest)\n",
    "#     print(f\"Test MSE for batch size {batch_size}: {mse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "8bee5cb6-666b-4728-9d84-94e2b3d8bcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u42222023 g64596037 5.815063017192867 3.181687\n",
      "Test MSE: 3.0494325160980225\n"
     ]
    }
   ],
   "source": [
    "u,i,r = interactionsTest[0]\n",
    "print(u,i,r,modelLFM.predict(userIDs[u], itemIDs[i]).numpy())\n",
    "def calculate_mse(model, interactions):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error for the given interactions.\n",
    "\n",
    "    Args:\n",
    "    model: The LatentFactorModel instance.\n",
    "    interactions: A list of tuples (user, item, rating).\n",
    "\n",
    "    Returns:\n",
    "    float: The mean squared error.\n",
    "    \"\"\"\n",
    "    total_squared_error = 0\n",
    "    count = 0\n",
    "\n",
    "    for u, i, r in interactions:\n",
    "        # Convert user and item to the indices used in the model\n",
    "        u_index = userIDs[u]\n",
    "        i_index = itemIDs[i]\n",
    "\n",
    "        # Predict the rating\n",
    "        predicted_rating = model.predict(u_index, i_index)\n",
    "        \n",
    "        # Compute squared error\n",
    "        squared_error = tf.square(predicted_rating - r)\n",
    "        total_squared_error += squared_error\n",
    "        count += 1\n",
    "\n",
    "    # Compute mean squared error\n",
    "    mse = total_squared_error / count\n",
    "    return mse.numpy()  # Convert to a regular Python number\n",
    "\n",
    "# Example usage\n",
    "mse_test = calculate_mse(modelLFM, interactionsTest)\n",
    "print(f\"Test MSE: {mse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "a4e8638f-8e28-4bee-b9bf-664054c8b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.32 0.001 75, 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119106b-b516-43bf-93b4-acb6a9b62541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.22 0.1 75, 0.0001\n",
    "#3.224193811416626 0.1 65, 0.0001\n",
    "#3.224165439605713 0.1 55, 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ced778-7aab-49be-a4bf-249f0bcbe496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2026658058166504 \n",
    "# K=1\n",
    "# lamb_beta=0.0001\n",
    "# lamb_gamma=0.0001\n",
    "# learning_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dbaf5-6834-47f5-b4a5-b31f9d82007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.17633056640625\n",
    "# K=1\n",
    "# lamb_beta=0.0001\n",
    "# lamb_gamma=0.001\n",
    "# learning_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5783fb-87b6-42de-beb8-ac6c0d7c11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1763246059417725\n",
    "# K=1\n",
    "# lamb_beta=0.0001\n",
    "# lamb_gamma=0.0005\n",
    "# learning_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991e4c0-c213-413f-965d-c9059207eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.081282377243042\n",
    "# K=1\n",
    "# lamb_beta=0.00005\n",
    "# lamb_gamma=0.0005\n",
    "# learning_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7cb2e8-2fd8-4679-a0d2-cd43f686b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.049272060394287\n",
    "# K=1\n",
    "# lamb_beta=0.00001\n",
    "# lamb_gamma=0.0005\n",
    "# learning_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266c423-a872-468b-b767-5e9ec014d1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed18ec-d137-445c-ba33-669223aab8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "24960239-4e51-43bc-ab78-4914abfc0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"/Users/zhiqiaogong/Projects/JupyterNotebook/cse258/assignment1/test/predictions_Hours.csv\", 'w')\n",
    "for l in open(\"/Users/zhiqiaogong/Projects/JupyterNotebook/cse258/hw3/pairs_Hours.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    # bu = 0\n",
    "    # bi = 0\n",
    "    # if u in betaU:\n",
    "    #     bu = betaU[u]\n",
    "    # if g in betaI:\n",
    "    #     bi = betaI[g]\n",
    "    p = modelLFM.predict(userIDs[u], itemIDs[g]).numpy()\n",
    "    _ = predictions.write(u + ',' + g + ',' + str(p) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ee1eddbb-49d0-4190-bcfa-832654449d06",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "trainingStep() missing 2 required positional arguments: 'val_data' and 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mtrainingStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelLFM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteractionsTrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m): \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, objective = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(obj))\n",
      "\u001b[0;31mTypeError\u001b[0m: trainingStep() missing 2 required positional arguments: 'val_data' and 'optimizer'"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    obj = trainingStep(modelLFM, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "005c1a02-c5bf-4241-8d00-dc260d36f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb2abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e54fa48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        g = d['gameID']\n",
    "        yield u,g,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c215087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27aec54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data structures that will be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e72d24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "allHours = []\n",
    "for l in readJSON(\"/Users/zhiqiaogong/Projects/JupyterNotebook/cse258/hw3/train.json.gz\"):\n",
    "    allHours.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "462bd9bd-b657-4571-8fef-eee11d852099",
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursTrain = allHours[:165000]\n",
    "hoursValid = allHours[165000:]\n",
    "hoursPerUser = defaultdict(list)\n",
    "hoursPerItem = defaultdict(list)\n",
    "for u,g,d in hoursTrain:\n",
    "    r = d['hours_transformed']\n",
    "    hoursPerUser[u].append((g,r))\n",
    "    hoursPerItem[g].append((u,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c58fd9e5-0ba6-4fef-83c1-315503d75348",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Play prediction                                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26eaa4dd-bef1-4baa-9a20-8d06b58637f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23ce9f0d-11ae-418f-bb1c-2cc5519ab2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "userSet = set()\n",
    "gameSet = set()\n",
    "playedSet = set()\n",
    "\n",
    "for u,g,d in allHours:\n",
    "    userSet.add(u)\n",
    "    gameSet.add(g)\n",
    "    playedSet.add((u,g))\n",
    "\n",
    "lUserSet = list(userSet)\n",
    "lGameSet = list(gameSet)\n",
    "\n",
    "notPlayedValid = set()\n",
    "for u,g,d in hoursValid:\n",
    "    g = random.choice(lGameSet)\n",
    "    while (u,g) in playedSet or (u,g) in notPlayedValid:\n",
    "        g = random.choice(lGameSet)\n",
    "    notPlayedValid.add((u,g))\n",
    "\n",
    "playedValid = set()\n",
    "for u,g,r in hoursValid:\n",
    "    playedValid.add((u,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "862b555c-b29c-4bd1-87e8-7ae905598a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction score: -0.017762354532299417\n"
     ]
    }
   ],
   "source": [
    "user_ids = {u: i for i, u in enumerate(userSet)}\n",
    "game_ids = {g: i for i, g in enumerate(gameSet)}\n",
    "\n",
    "num_users = len(user_ids)\n",
    "num_games = len(game_ids)\n",
    "\n",
    "# Initialize the user-item matrix with zeros\n",
    "user_item_matrix = np.zeros((num_users, num_games))\n",
    "\n",
    "for u, g, _ in hoursTrain:\n",
    "    user_item_matrix[user_ids[u], game_ids[g]] = 1\n",
    "\n",
    "# BPR Model\n",
    "class BPR:\n",
    "    def __init__(self, num_users, num_items, num_factors=10):\n",
    "        self.user_factors = np.random.normal(0, 0.1, (num_users, num_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (num_items, num_factors))\n",
    "    \n",
    "    def predict(self, user, item):\n",
    "        return np.dot(self.user_factors[user], self.item_factors[item])\n",
    "    \n",
    "    def train(self, matrix, epochs=100, learning_rate=0.01, lambda_reg=0.01):\n",
    "         # Initialize Adam optimizer parameters\n",
    "        beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n",
    "        m_user, v_user = np.zeros(self.user_factors.shape), np.zeros(self.user_factors.shape)\n",
    "        m_item, v_item = np.zeros(self.item_factors.shape), np.zeros(self.item_factors.shape)\n",
    "        t = 0\n",
    "        for _ in range(epochs):\n",
    "            t += 1\n",
    "            user = np.random.randint(num_users)\n",
    "            pos_items = np.where(matrix[user] == 1)[0]\n",
    "            neg_items = self.get_neg_item_candidates(user, matrix, pos_items)\n",
    "\n",
    "            if not neg_items:\n",
    "                continue\n",
    "\n",
    "            pos_item = np.random.choice(pos_items)\n",
    "            neg_item = np.random.choice(neg_items)\n",
    "\n",
    "            \n",
    "            \n",
    "            # Calculate error and gradients\n",
    "            x_uij = self.predict(user, pos_item) - self.predict(user, neg_item)\n",
    "            exp_x = np.exp(-x_uij)\n",
    "            loss = exp_x / (1 + exp_x)\n",
    "\n",
    "            user_gradient = (self.item_factors[pos_item] - self.item_factors[neg_item]) * loss - lambda_reg * self.user_factors[user]\n",
    "            item_pos_gradient = self.user_factors[user] * loss - lambda_reg * self.item_factors[pos_item]\n",
    "            item_neg_gradient = -self.user_factors[user] * loss - lambda_reg * self.item_factors[neg_item]\n",
    "\n",
    "            # Update user and item factors using Adam optimizer\n",
    "            m_user = beta1 * m_user + (1 - beta1) * user_gradient\n",
    "            v_user = beta2 * v_user + (1 - beta2) * (user_gradient ** 2)\n",
    "            m_item[pos_item] = beta1 * m_item[pos_item] + (1 - beta1) * item_pos_gradient\n",
    "            v_item[pos_item] = beta2 * v_item[pos_item] + (1 - beta2) * (item_pos_gradient ** 2)\n",
    "            m_item[neg_item] = beta1 * m_item[neg_item] + (1 - beta1) * item_neg_gradient\n",
    "            v_item[neg_item] = beta2 * v_item[neg_item] + (1 - beta2) * (item_neg_gradient ** 2)\n",
    "\n",
    "            m_user_corr = m_user / (1 - beta1 ** t)\n",
    "            v_user_corr = v_user / (1 - beta2 ** t)\n",
    "            m_item_corr = m_item / (1 - beta1 ** t)\n",
    "            v_item_corr = v_item / (1 - beta2 ** t)\n",
    "\n",
    "            self.user_factors -= learning_rate * m_user_corr / (np.sqrt(v_user_corr) + epsilon)\n",
    "            self.item_factors[pos_item] -= learning_rate * m_item_corr[pos_item] / (np.sqrt(v_item_corr[pos_item]) + epsilon)\n",
    "            self.item_factors[neg_item] -= learning_rate * m_item_corr[neg_item] / (np.sqrt(v_item_corr[neg_item]) + epsilon)\n",
    "\n",
    "    def get_neg_item_candidates(self, user, matrix, pos_items):\n",
    "        # Implement a logic to select negative item candidates\n",
    "        # Example: Choose items not interacted with by the user and are popular among other users\n",
    "        # Return a list of item indices\n",
    "        all_items = set(range(matrix.shape[1]))\n",
    "        neg_candidates = list(all_items - set(pos_items))\n",
    "        return neg_candidates\n",
    "\n",
    "# Train the BPR model\n",
    "bpr_model = BPR(num_users, num_games)\n",
    "bpr_model.train(user_item_matrix)\n",
    "\n",
    "# Example prediction (You can replace these with actual user and game IDs from your data)\n",
    "user_id = 0  # example user index\n",
    "game_id = 10  # example game index\n",
    "prediction = bpr_model.predict(user_id, game_id)\n",
    "print(\"Prediction score:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7dcfcce-efa2-4284-a9c7-2cc99a2c7299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Validation Set: 0.5037769518840333\n"
     ]
    }
   ],
   "source": [
    "# Function to predict for a user-item pair\n",
    "def predict_bpr(bpr_model, user_id, item_id):\n",
    "    return bpr_model.predict(user_ids[user_id], game_ids[item_id])\n",
    "\n",
    "# Calculate predictions for the validation set and compute MSE\n",
    "def calculate_mse(bpr_model, played_valid, not_played_valid):\n",
    "    mse = 0\n",
    "    count = 0\n",
    "\n",
    "    # For pairs in playedValid, the actual interaction is 1\n",
    "    for (u, g) in played_valid:\n",
    "        predicted_score = predict_bpr(bpr_model, u, g)\n",
    "        mse += (predicted_score - 1) ** 2  # actual interaction is 1\n",
    "        count += 1\n",
    "\n",
    "    # For pairs in notPlayedValid, the actual interaction is 0\n",
    "    for (u, g) in not_played_valid:\n",
    "        predicted_score = predict_bpr(bpr_model, u, g)\n",
    "        mse += (predicted_score - 0) ** 2  # actual interaction is 0\n",
    "        count += 1\n",
    "\n",
    "    return mse / count if count > 0 else 0\n",
    "\n",
    "# Calculate MSE on the validation set\n",
    "mse_validation = calculate_mse(bpr_model, playedValid, notPlayedValid)\n",
    "print(\"MSE on Validation Set:\", mse_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5219fa42-3a97-40a4-8b44-c8e2881f52aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Validation Set: 0.5\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(bpr_model, played_valid, not_played_valid, threshold=0.5):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(played_valid) + len(not_played_valid)\n",
    "\n",
    "    # Check predictions for playedValid (should be predicted as played, i.e., score > threshold)\n",
    "    for (u, g) in played_valid:\n",
    "        predicted_score = predict_bpr(bpr_model, u, g)\n",
    "        if predicted_score > threshold:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    # Check predictions for notPlayedValid (should be predicted as not played, i.e., score <= threshold)\n",
    "    for (u, g) in not_played_valid:\n",
    "        predicted_score = predict_bpr(bpr_model, u, g)\n",
    "        if predicted_score <= threshold:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Accuracy on the validation set\n",
    "accuracy_validation = calculate_accuracy(bpr_model, playedValid, notPlayedValid)\n",
    "print(\"Accuracy on Validation Set:\", accuracy_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b87d233-1317-4422-9b17-c99696d16fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47554755475547555\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.49884988498849886\n",
      "0.4986998699869987\n",
      "0.4881988198819882\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.4936993699369937\n",
      "0.49064906490649063\n",
      "0.46744674467446745\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.49004900490049\n",
      "0.49744974497449745\n",
      "Best Accuracy: 0.5001500150015001\n",
      "Best Hyperparameters: {'factors': 30, 'learning_rate': 0.05, 'regularization': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# List of hyperparameters to try\n",
    "factor_options = [10, 20, 30]  # Number of latent factors\n",
    "learning_rate_options = [0.005, 0.01, 0.05]\n",
    "regularization_options = [0.001, 0.01, 0.1]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for factors in factor_options:\n",
    "    for lr in learning_rate_options:\n",
    "        for reg in regularization_options:\n",
    "            # Initialize and train the BPR model\n",
    "            bpr_model = BPR(num_users, num_games, num_factors=factors)\n",
    "            bpr_model.train(user_item_matrix, learning_rate=lr, lambda_reg=reg)\n",
    "\n",
    "            print(current_accuracy)\n",
    "            # Evaluate the model\n",
    "            current_accuracy = calculate_accuracy(bpr_model, playedValid, notPlayedValid)\n",
    "            \n",
    "            # Update best parameters if current model is better\n",
    "            if current_accuracy > best_accuracy:\n",
    "                best_accuracy = current_accuracy\n",
    "                best_params = {'factors': factors, 'learning_rate': lr, 'regularization': reg}\n",
    "\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "1d97c2dc-cee0-4a6e-b0ed-db3cde9a881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer/denom\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "9ce7d7d0-f73e-46fa-b764-b796b1c79a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "userSet = set()\n",
    "gameSet = set()\n",
    "playedSet = set()\n",
    "\n",
    "for u,g,d in allHours:\n",
    "    userSet.add(u)\n",
    "    gameSet.add(g)\n",
    "    playedSet.add((u,g))\n",
    "\n",
    "lUserSet = list(userSet)\n",
    "lGameSet = list(gameSet)\n",
    "\n",
    "notPlayedValid = set()\n",
    "for u,g,d in hoursValid:\n",
    "    g = random.choice(lGameSet)\n",
    "    while (u,g) in playedSet or (u,g) in notPlayedValid:\n",
    "        g = random.choice(lGameSet)\n",
    "    notPlayedValid.add((u,g))\n",
    "\n",
    "playedValid = set()\n",
    "for u,g,r in hoursValid:\n",
    "    playedValid.add((u,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "67bf7c08-13cb-4488-a921-ee0a3ca538ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "popThreshold = 2/3\n",
    "simPlayedThreshold = 0.0235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "54e5c4f7-f846-4693-b272-f8e405bb9d1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "popTh() missing 1 required positional argument: 'jth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[378], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m mses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m thpop:\n\u001b[0;32m----> 4\u001b[0m     mses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpopTh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: popTh() missing 1 required positional argument: 'jth'"
     ]
    }
   ],
   "source": [
    "thpop = [i/30 for i in range(15,30)]\n",
    "mses = []\n",
    "for t in thpop:\n",
    "    mses.append(popTh(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2536d49-eaa9-4e85-8bd2-9a53fcbf0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "thjaccards = [1*i/10000 for i in range(230,350)]\n",
    "mses = []\n",
    "for t in thjaccards:\n",
    "    mses.append(popJaccardPredict(2/3,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "4e06cc33-bc60-4b45-be63-8033c17d9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popJaccardPredict(popth,jth):\n",
    "    gameCount = defaultdict(int)\n",
    "    totalPlayed = 0\n",
    "    \n",
    "    for u,g,_ in hoursTrain:\n",
    "        gameCount[g] += 1\n",
    "        totalPlayed += 1\n",
    "    \n",
    "    mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "    mostPopular.sort()\n",
    "    mostPopular.reverse()\n",
    "    \n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > popth * totalPlayed: break\n",
    "    \n",
    "    predictions = 0\n",
    "    for (label,sample) in [(1, playedValid), (0, notPlayedValid)]:\n",
    "        for (u,g) in sample:\n",
    "            maxJaccard = 0\n",
    "            users = set(hoursPerItem[g])\n",
    "            for g2,_ in hoursPerUser[u]:\n",
    "                sim = Jaccard(users,set(hoursPerItem[g2]))\n",
    "                if sim > maxJaccard:\n",
    "                    maxJaccard = sim\n",
    "            if maxJaccard > jth or g in return1:\n",
    "                pred = 1\n",
    "            else:\n",
    "                pred = 0\n",
    "            if pred == label:\n",
    "                predictions += 1\n",
    "    print(str(predictions / (len(playedValid) + len(notPlayedValid)))+\" \"+str(jth))\n",
    "    return predictions / (len(playedValid) + len(notPlayedValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "13118fe1-ad9a-49df-a747-035aae517527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7025702570257025 0.0235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7025702570257025"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popJaccardPredict(popThreshold, simPlayedThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "75f81286-487d-494a-8ee8-a42c1aca6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Played.csv\", 'w')\n",
    "for l in open(\"/Users/zhiqiaogong/Projects/JupyterNotebook/cse258/hw3/pairs_Played.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > popThreshold * totalPlayed: break\n",
    "    \n",
    "    maxJaccard = 0\n",
    "    users = set(hoursPerItem[g])\n",
    "    for g2,_ in hoursPerUser[u]:\n",
    "        sim = Jaccard(users,set(hoursPerItem[g2]))\n",
    "        if sim > maxJaccard:\n",
    "            maxJaccard = sim\n",
    "    if maxJaccard > simPlayedThreshold or g in return1:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "    _ = predictions.write(u + ',' + g + ',' + str(pred) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c82a7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Hours played prediction                        #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "772dd561-ceae-4c2e-9347-7ba3eb2dd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHours = [r[2]['hours_transformed'] for r in hoursTrain]\n",
    "globalAverage = sum(trainHours) * 1.0 / len(trainHours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "26ebd33a-e465-4544-a7dd-1fe5e1e98c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UsersGames(data):\n",
    "    usergames = defaultdict(set)\n",
    "    gameusers = defaultdict(set)\n",
    "    for u, g, d in data:\n",
    "        usergames[u].add(g)\n",
    "        gameusers[g].add(u)\n",
    "    return usergames, gameusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "a744b67c-d7b9-4be1-8e5f-86dc6dd410f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "usergames_train,gameusers_train = UsersGames(hoursTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "989b0fa2-6cc0-4f46-8a18-d4384724c57d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[234], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m alpha \u001b[38;5;241m=\u001b[39m globalAverage  \u001b[38;5;66;03m# This is the global average we calculated earlier\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[234], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(lamb, iterations, learning_rate, tolerance)\u001b[0m\n\u001b[1;32m     21\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (u, g), r \u001b[38;5;129;01min\u001b[39;00m usergameHours\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 24\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     error \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m-\u001b[39m prediction\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Update biases\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[234], line 11\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(u, g)\u001b[0m\n\u001b[1;32m      8\u001b[0m gamma_u \u001b[38;5;241m=\u001b[39m {u: [random() \u001b[38;5;241m/\u001b[39m sqrt(num_features) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features)] \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m hoursPerUser}\n\u001b[1;32m      9\u001b[0m gamma_i \u001b[38;5;241m=\u001b[39m {g: [random() \u001b[38;5;241m/\u001b[39m sqrt(num_features) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features)] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m hoursPerItem}\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(u, g):\n\u001b[1;32m     12\u001b[0m     dot_product \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(gamma_u[u][k] \u001b[38;5;241m*\u001b[39m gamma_i[g][k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m alpha \u001b[38;5;241m+\u001b[39m betaU\u001b[38;5;241m.\u001b[39mget(u, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m betaI\u001b[38;5;241m.\u001b[39mget(g, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m dot_product\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize parameters\n",
    "num_features = 10  # This is an example, can be tuned\n",
    "\n",
    "# Latent feature matrices\n",
    "gamma_u = {u: [random() / sqrt(num_features) for _ in range(num_features)] for u in hoursPerUser}\n",
    "gamma_i = {g: [random() / sqrt(num_features) for _ in range(num_features)] for g in hoursPerItem}\n",
    "\n",
    "def predict(u, g):\n",
    "    dot_product = sum(gamma_u[u][k] * gamma_i[g][k] for k in range(num_features))\n",
    "    return alpha + betaU.get(u, 0) + betaI.get(g, 0) + dot_product\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "d4b39158-3b06-4057-bfb7-5ef1d547e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "betaU = {}\n",
    "betaI = {}\n",
    "for u in hoursPerUser:\n",
    "    betaU[u] = 0\n",
    "\n",
    "for g in hoursPerItem:\n",
    "    betaI[g] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "c11d604a-e983-46b6-ae06-71931e04e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = globalAverage # Could initialize anywhere, this is a guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "be6e8d4f-acbf-4d2d-b504-d5c513bfa99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 1\n",
    "iter = 100\n",
    "tolerance = 1e-5 \n",
    "pmse = 0\n",
    "converged = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b9a797dc-7902-4ba6-9c08-89a24f0aa7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "usergameHours = {}\n",
    "for u, g, d in hoursTrain:\n",
    "    usergameHours[(u,g)] = d['hours_transformed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d872b969-b0dc-46ed-afae-e7fe0387cac3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'validation_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m alpha \u001b[38;5;241m=\u001b[39m globalAverage  \u001b[38;5;66;03m# This is the global average we calculated earlier\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [predict(u,g) \u001b[38;5;28;01mfor\u001b[39;00m u,g,d \u001b[38;5;129;01min\u001b[39;00m hoursValid]\n\u001b[1;32m     75\u001b[0m y \u001b[38;5;241m=\u001b[39m [d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhours_transformed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m u,g,d \u001b[38;5;129;01min\u001b[39;00m hoursValid]\n",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'validation_data'"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize parameters\n",
    "num_features = 10  # This is an example, can be tuned\n",
    "\n",
    "# Latent feature matrices\n",
    "gamma_u = {u: [random() / sqrt(num_features) for _ in range(num_features)] for u in hoursPerUser}\n",
    "gamma_i = {g: [random() / sqrt(num_features) for _ in range(num_features)] for g in hoursPerItem}\n",
    "\n",
    "def predict(u, g):\n",
    "    dot_product = sum(gamma_u[u][k] * gamma_i[g][k] for k in range(num_features))\n",
    "    return alpha + betaU.get(u, 0) + betaI.get(g, 0) + dot_product\n",
    "\n",
    "def train(lamb, iterations, learning_rate, tolerance, early_stopping_rounds):\n",
    "    global alpha, betaU, betaI, gamma_u, gamma_i\n",
    "    \n",
    "    last_loss = float('inf')\n",
    "    best_loss = float('inf')\n",
    "    loss_increasing_rounds = 0\n",
    "    learning_rate_schedule = learning_rate\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        total_loss = 0\n",
    "        learning_rate_schedule *= (0.99 ** iteration)  # Gradually decrease learning rate\n",
    "        \n",
    "        for (u, g), r in usergameHours.items():\n",
    "            prediction = predict(u, g)\n",
    "            error = r - prediction\n",
    "\n",
    "            # Update biases\n",
    "            betaU[u] = betaU.get(u, 0) + learning_rate_schedule * (error - lamb * betaU.get(u, 0))\n",
    "            betaI[g] = betaI.get(g, 0) + learning_rate_schedule * (error - lamb * betaI.get(g, 0))\n",
    "\n",
    "            # Update latent factors\n",
    "            for k in range(num_features):\n",
    "                pu = gamma_u[u][k]\n",
    "                qi = gamma_i[g][k]\n",
    "\n",
    "                gamma_u[u][k] = pu + learning_rate_schedule * (error * qi - lamb * pu)\n",
    "                gamma_i[g][k] = qi + learning_rate_schedule * (error * pu - lamb * qi)\n",
    "            \n",
    "            total_loss += error ** 2\n",
    "\n",
    "        # Regularization term for biases and latent factors\n",
    "        for u in betaU:\n",
    "            total_loss += lamb * betaU[u] ** 2\n",
    "        for g in betaI:\n",
    "            total_loss += lamb * betaI[g] ** 2\n",
    "        for u in gamma_u:\n",
    "            for k in range(num_features):\n",
    "                total_loss += lamb * gamma_u[u][k] ** 2\n",
    "        for g in gamma_i:\n",
    "            for k in range(num_features):\n",
    "                total_loss += lamb * gamma_i[g][k] ** 2\n",
    "\n",
    "        print(total_loss\n",
    "        # Early stopping condition\n",
    "        if total_loss > last_loss:\n",
    "            loss_increasing_rounds += 1\n",
    "            if loss_increasing_rounds >= early_stopping_rounds:\n",
    "                print(f'Stopping early at iteration {iteration}')\n",
    "                break\n",
    "        else:\n",
    "            loss_increasing_rounds = 0\n",
    "            if total_loss < best_loss:\n",
    "                best_loss = total_loss\n",
    "                # Save the best parameters if necessary\n",
    "        \n",
    "        if abs(last_loss - total_loss) < tolerance:\n",
    "            print(f'Converged at iteration {iteration}')\n",
    "            break\n",
    "        \n",
    "        last_loss = total_loss\n",
    "    \n",
    "    return betaU, betaI, gamma_u, gamma_i, best_loss\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the biases\n",
    "betaU = {u: 0 for u in hoursPerUser}\n",
    "betaI = {g: 0 for g in hoursPerItem}\n",
    "alpha = globalAverage  # This is the global average we calculated earlier\n",
    "\n",
    "# Run training\n",
    "train(lamb=0.1, iterations=1000, learning_rate=0.005, tolerance=1e-4, early_stopping_rounds=5)\n",
    "\n",
    "predictions = [predict(u,g) for u,g,d in hoursValid]\n",
    "y = [d['hours_transformed'] for u,g,d in hoursValid]\n",
    "\n",
    "mse = mean_squared_error(predictions, y)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "4aa384a0-1617-460b-9a83-0b90e83c9d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "3.9713339312923095\n",
      "3.601151212561402\n",
      "3.413654011341416\n",
      "3.2999124668385167\n",
      "3.2248003036521764\n",
      "3.172824859148101\n",
      "3.1359821046422702\n",
      "3.1096606559557083\n",
      "3.0909434376466884\n",
      "3.07782572498686\n",
      "3.0688426424422675\n",
      "3.062890994314282\n",
      "3.059138911876705\n",
      "3.0569703485771416\n",
      "3.05594255988193\n",
      "3.055750656556877\n",
      "3.055750656556877\n",
      "3.055750656556877\n",
      "3.055750656556877\n",
      "3.055750656556877\n",
      "Stopping early at iteration 20\n",
      "Best validation loss: 3.055750656556877\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize parameters\n",
    "num_features = 25  # This is an example, can be tuned\n",
    "\n",
    "# Latent feature matrices\n",
    "gamma_u = {u: [random() / sqrt(num_features) for _ in range(num_features)] for u in hoursPerUser}\n",
    "gamma_i = {g: [random() / sqrt(num_features) for _ in range(num_features)] for g in hoursPerItem}\n",
    "\n",
    "# Predict function\n",
    "def predict(u, g):\n",
    "    dot_product = sum(gamma_u[u][k] * gamma_i[g][k] for k in range(num_features))\n",
    "    return alpha + betaU.get(u, 0) + betaI.get(g, 0) + dot_product\n",
    "\n",
    "# Function to calculate the loss on validation set\n",
    "def calculate_validation_loss(validation_data, lamb):\n",
    "    validation_loss = 0\n",
    "    for u, g, r in validation_data:\n",
    "        prediction = predict(u, g)\n",
    "        error = r - prediction\n",
    "        validation_loss += error ** 2\n",
    "    \n",
    "    # Regularization term\n",
    "    for u in betaU:\n",
    "        validation_loss += lamb * betaU[u] ** 2\n",
    "    for g in betaI:\n",
    "        validation_loss += lamb * betaI[g] ** 2\n",
    "    for u in gamma_u:\n",
    "        for k in range(num_features):\n",
    "            validation_loss += lamb * gamma_u[u][k] ** 2\n",
    "    for g in gamma_i:\n",
    "        for k in range(num_features):\n",
    "            validation_loss += lamb * gamma_i[g][k] ** 2\n",
    "    \n",
    "    return validation_loss / len(validation_data)\n",
    "\n",
    "# Training function\n",
    "def train(lamb, iterations, initial_learning_rate, tolerance, early_stopping_rounds, validation_data):\n",
    "    global alpha, betaU, betaI, gamma_u, gamma_i\n",
    "\n",
    "    learning_rate = initial_learning_rate\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    best_betaU, best_betaI, best_gamma_u, best_gamma_i = {}, {}, {}, {}\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        total_loss = 0\n",
    "\n",
    "        learning_rate = learning_rate / (1 + decay_rate * iteration)\n",
    "\n",
    "        for (u, g), r in usergameHours.items():\n",
    "            prediction = predict(u, g)\n",
    "            error = r - prediction\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            # Update biases\n",
    "            betaU[u] = betaU.get(u, 0) + learning_rate * (error - lamb * betaU.get(u, 0))\n",
    "            betaI[g] = betaI.get(g, 0) + learning_rate * (error - lamb * betaI.get(g, 0))\n",
    "\n",
    "            # Update latent factors\n",
    "            for k in range(num_features):\n",
    "                pu = gamma_u[u][k]\n",
    "                qi = gamma_i[g][k]\n",
    "\n",
    "                gamma_u[u][k] = pu + learning_rate * (error * qi - lamb * pu)\n",
    "                gamma_i[g][k] = qi + learning_rate * (error * pu - lamb * qi)\n",
    "            \n",
    "            total_loss += error ** 2\n",
    "\n",
    "        # Regularization term for biases and latent factors\n",
    "        for u in betaU:\n",
    "            total_loss += lamb * betaU[u] ** 2\n",
    "        for g in betaI:\n",
    "            total_loss += lamb * betaI[g] ** 2\n",
    "        for u in gamma_u:\n",
    "            for k in range(num_features):\n",
    "                total_loss += lamb * gamma_u[u][k] ** 2\n",
    "        for g in gamma_i:\n",
    "            for k in range(num_features):\n",
    "                total_loss += lamb * gamma_i[g][k] ** 2\n",
    "        \n",
    "        validation_loss = calculate_validation_loss(validation_data, lamb)\n",
    "\n",
    "        print(best_loss)\n",
    "        # Check if validation loss improved\n",
    "        if validation_loss < best_loss:\n",
    "            best_loss = validation_loss\n",
    "            best_betaU, best_betaI, best_gamma_u, best_gamma_i = betaU.copy(), betaI.copy(), gamma_u.copy(), gamma_i.copy()\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        if no_improvement_count >= early_stopping_rounds:\n",
    "            print(f'Stopping early at iteration {iteration}')\n",
    "            break\n",
    "\n",
    "        if abs(total_loss - validation_loss) < tolerance:\n",
    "            print(f'Converged at iteration {iteration}')\n",
    "            break\n",
    "    \n",
    "    return best_betaU, best_betaI, best_gamma_u, best_gamma_i, best_loss\n",
    "\n",
    "# Initialize the biases\n",
    "alpha = globalAverage\n",
    "betaU = {u: random() * 0.1 for u in hoursPerUser}\n",
    "betaI = {g: random() * 0.1 for g in hoursPerItem}\n",
    "\n",
    "# Validation data\n",
    "validation_data = [(u, g, d['hours_transformed']) for u, g, d in hoursValid]\n",
    "\n",
    "decay_rate = 0.001\n",
    "# Run training with early stopping\n",
    "best_betaU, best_betaI, best_gamma_u, best_gamma_i, best_loss = train(\n",
    "    lamb=0.01, \n",
    "    iterations=1000, \n",
    "    initial_learning_rate=0.003, \n",
    "    tolerance=1e-5, \n",
    "    early_stopping_rounds=5, \n",
    "    validation_data=validation_data\n",
    ")\n",
    "\n",
    "print(f'Best validation loss: {best_loss}')\n",
    "#0.001, 0.003 3.0531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "2f63c99a-765e-4024-8423-9df6be3bb92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0589051556535334"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [predict(u,g) for u,g,d in hoursValid]\n",
    "y = [d['hours_transformed'] for u,g,d in hoursValid]\n",
    "\n",
    "mse = mean_squared_error(predictions, y)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "37c91b20-5350-431e-954b-15e097a1e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model with 5 features...\n",
      "inf\n",
      "3.978091998286789\n",
      "3.605851378010019\n",
      "3.4174208409636275\n",
      "3.3029957163115733\n",
      "3.2272537247332393\n",
      "3.174652490353369\n",
      "3.137182288336349\n",
      "3.1102485740072567\n",
      "3.0909688015816608\n",
      "3.077386097561197\n",
      "3.0680902071894414\n",
      "3.0620322598926513\n",
      "3.058429002824698\n",
      "3.0567043229222737\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "3.0564453875369932\n",
      "Stopping early at iteration 34\n",
      "Evaluating model with 10 features...\n",
      "inf\n",
      "3.965387070953643\n",
      "3.5962943835478676\n",
      "3.4095383720787695\n",
      "3.29621958975174\n",
      "3.221318244299703\n",
      "3.1694227628603637\n",
      "3.1325842474136714\n",
      "3.10622785067199\n",
      "3.087460639242577\n",
      "3.074295558423495\n",
      "3.0652838077643585\n",
      "3.0593389032149605\n",
      "3.0556455651366883\n",
      "3.0536017848121433\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "3.0527738561973017\n",
      "Stopping early at iteration 34\n",
      "Evaluating model with 15 features...\n",
      "inf\n",
      "3.9668034509591523\n",
      "3.5967068183644897\n",
      "3.409453128089322\n",
      "3.2960708586291196\n",
      "3.221342875354254\n",
      "3.1697409986209544\n",
      "3.1332477463578203\n",
      "3.107246438160367\n",
      "3.088818760846821\n",
      "3.0759625687642256\n",
      "3.0672203917419645\n",
      "3.0615020363696925\n",
      "3.0579940499184013\n",
      "3.056103047656719\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "3.0554112166588143\n",
      "Stopping early at iteration 34\n",
      "Evaluating model with 20 features...\n",
      "inf\n",
      "3.9707506707842133\n",
      "3.6024325667987283\n",
      "3.415767428506201\n",
      "3.3024286770392814\n",
      "3.2275019044432836\n",
      "3.1756019488579335\n",
      "3.1387844237396463\n",
      "3.1124703299345464\n",
      "3.0937570220261206\n",
      "3.0806430362941315\n",
      "3.071663230029917\n",
      "3.0657187811581896\n",
      "3.0619934674647875\n",
      "3.059901593569133\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "3.0590451830387737\n",
      "Stopping early at iteration 34\n",
      "Evaluating model with 25 features...\n",
      "inf\n",
      "3.9686651793580348\n",
      "3.5991865802399876\n",
      "3.411859997404557\n",
      "3.2980714067483325\n",
      "3.222781858293284\n",
      "3.1705515355925713\n",
      "3.133419848075064\n",
      "3.1068134287749714\n",
      "3.0878446867880838\n",
      "3.0745245046789713\n",
      "3.065386839465358\n",
      "3.059312676194146\n",
      "3.0554458466596754\n",
      "3.053144689319911\n",
      "3.051945057587447\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "3.051527665578001\n",
      "Stopping early at iteration 35\n",
      "Evaluating model with 30 features...\n",
      "inf\n",
      "3.9687254921829678\n",
      "3.598356140063828\n",
      "3.4106440393695974\n",
      "3.2967233169527113\n",
      "3.2214549191925284\n",
      "3.1693467585914044\n",
      "3.132398130104876\n",
      "3.1059989485019552\n",
      "3.087229508649105\n",
      "3.074078252583496\n",
      "3.0650707766529033\n",
      "3.059094736873067\n",
      "3.055313168414174\n",
      "3.0531120507891027\n",
      "3.052059344590645\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "3.0518690162995235\n",
      "Stopping early at iteration 35\n",
      "Best number of features: 25 with validation loss: 3.135655923026967\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(num_features, lamb, initial_learning_rate, iterations, tolerance, early_stopping_rounds, train_data, validation_data):\n",
    "    global alpha, betaU, betaI, gamma_u, gamma_i\n",
    "    \n",
    "    # Initialize the latent factors with the new number of features\n",
    "    alpha = sum([r for _, _, r in train_data]) / len(train_data)\n",
    "    betaU = {u: random() * 0.1 for u, _, _ in train_data}\n",
    "    betaI = {g: random() * 0.1 for _, g, _ in train_data}\n",
    "    gamma_u = {u: [random() / sqrt(num_features) for _ in range(num_features)] for u, _, _ in train_data}\n",
    "    gamma_i = {g: [random() / sqrt(num_features) for _ in range(num_features)] for _, g, _ in train_data}\n",
    "    \n",
    "    # Train the model\n",
    "    best_betaU, best_betaI, best_gamma_u, best_gamma_i, best_loss = train(\n",
    "        lamb=lamb, \n",
    "        iterations=iterations, \n",
    "        initial_learning_rate=initial_learning_rate, \n",
    "        tolerance=tolerance, \n",
    "        early_stopping_rounds=early_stopping_rounds, \n",
    "        validation_data=validation_data\n",
    "    )\n",
    "    \n",
    "    # Calculate the loss on the validation set\n",
    "    validation_loss = calculate_validation_loss(validation_data, lamb)\n",
    "    \n",
    "    return validation_loss\n",
    "\n",
    "# Define a range of feature counts to try\n",
    "feature_counts = [5, 10, 15, 20, 25, 30]\n",
    "validation_losses = []\n",
    "\n",
    "# Iterate over the range of feature counts\n",
    "for num_features in feature_counts:\n",
    "    print(f\"Evaluating model with {num_features} features...\")\n",
    "    loss = train_and_evaluate(\n",
    "        num_features=num_features,\n",
    "        lamb=0.01,\n",
    "        initial_learning_rate=0.003,\n",
    "        iterations=1000,\n",
    "        tolerance=1e-5,\n",
    "        early_stopping_rounds=20,\n",
    "        train_data=[(u, g, d['hours_transformed']) for u, g, d in hoursTrain],  # Replace with your actual training data\n",
    "        validation_data=[(u, g, d['hours_transformed']) for u, g, d in hoursValid]  # Replace with your actual validation data\n",
    "    )\n",
    "    validation_losses.append((num_features, loss))\n",
    "\n",
    "# Select the number of features with the lowest validation loss\n",
    "best_num_features, best_loss = min(validation_losses, key=lambda x: x[1])\n",
    "print(f'Best number of features: {best_num_features} with validation loss: {best_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "d6cde1f2-15ee-46c7-95e6-d17387fcbf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model with 25 features...\n",
      "inf\n",
      "2.3249410671631177\n",
      "2.271673289984262\n",
      "2.2166251336991953\n",
      "2.160643738744893\n",
      "2.1044541556112595\n",
      "2.048630446623668\n",
      "1.9935906706034774\n",
      "1.9396190511434726\n",
      "1.8869013269483634\n",
      "1.835558995494893\n",
      "1.7856750082482788\n",
      "1.7373099618913364\n",
      "1.6905108373525686\n",
      "1.6453147889321118\n",
      "1.6017500633160306\n",
      "1.5598356058454446\n",
      "1.5195804081815913\n",
      "1.4809831815612433\n",
      "1.4440325628964863\n",
      "1.408707821996579\n",
      "1.374979929267039\n",
      "1.3428128226424383\n",
      "1.3121647361098248\n",
      "1.2829894897338912\n",
      "1.25523767722717\n",
      "1.228857716251128\n",
      "1.2037967479761569\n",
      "1.1800013869179191\n",
      "1.157418331081469\n",
      "1.1359948473167332\n",
      "1.1156791487084692\n",
      "1.096420680793193\n",
      "1.078170332197211\n",
      "1.0608805835170227\n",
      "1.044505606306609\n",
      "1.029001322142164\n",
      "1.0143254300243574\n",
      "1.0004374089053292\n",
      "0.9872985008862761\n",
      "0.974871679611023\n",
      "0.9631216075351736\n",
      "0.9520145850645465\n",
      "0.941518493986275\n",
      "0.9316027371504494\n",
      "0.9222381759733733\n",
      "0.9133970670135427\n",
      "0.9050529986051903\n",
      "0.897180828316379\n",
      "0.8897566218152047\n",
      "0.8827575935799403\n",
      "0.8761620497654268\n",
      "0.8699493334398005\n",
      "0.8640997723245903\n",
      "0.8585946291067998\n",
      "0.8534160543414492\n",
      "0.8485470419218452\n",
      "0.8439713870649896\n",
      "0.839673646735328\n",
      "0.8356391024132355\n",
      "0.8318537251018611\n",
      "0.8283041424581232\n",
      "0.8249776079279133\n",
      "0.8218619717645156\n",
      "0.8189456538065502\n",
      "0.8162176178965378\n",
      "0.813667347820646\n",
      "0.8112848246560257\n",
      "0.8090605054159203\n",
      "0.8069853028887701\n",
      "0.8050505665717309\n",
      "0.8032480646065325\n",
      "0.8015699666310061\n",
      "0.8000088274658248\n",
      "0.7985575715628649\n",
      "0.7972094781467557\n",
      "0.7959581669889899\n",
      "0.7947975847575535\n",
      "0.7937219918934747\n",
      "0.7927259499683944\n",
      "0.7918043094848181\n",
      "0.7909521980837879\n",
      "0.7901650091307327\n",
      "0.7894383906529707\n",
      "0.7887682346074179\n",
      "0.7881506664602104\n",
      "0.787582035062856\n",
      "0.7870589028122279\n",
      "0.786578036085469\n",
      "0.7861363959406673\n",
      "0.7857311290785542\n",
      "0.7853595590608092\n",
      "0.7850191777809288\n",
      "0.7847076371869058\n",
      "0.7844227412536956\n",
      "0.784162438204918\n",
      "0.7839248129836561\n",
      "0.783708079972035\n",
      "0.7835105759597899\n",
      "0.7833307533612132\n",
      "0.7831671736807878\n",
      "0.7830185012259515\n",
      "0.782883497067325\n",
      "0.7827610132433318\n",
      "0.7826499872089033\n",
      "0.7825494365251319\n",
      "0.7824584537866011\n",
      "0.7823762017845424\n",
      "0.7823019089000732\n",
      "0.7822348647248966\n",
      "0.7821744159036375\n",
      "0.782119962193125\n",
      "0.7820709527328517\n",
      "0.7820268825210075\n",
      "0.7819872890894796\n",
      "0.7819517493716951\n",
      "0.7819198767565123\n",
      "0.7818913183213376\n",
      "0.7818657522370936\n",
      "0.7818428853388495\n",
      "0.7818224508539693\n",
      "0.7818042062814015\n",
      "0.7817879314145404\n",
      "0.7817734265006886\n",
      "0.7817605105304613\n",
      "0.781749019649535\n",
      "0.7817388056872796\n",
      "0.7817297347936968\n",
      "0.7817216861806872\n",
      "0.7817145509595458\n",
      "0.7817082310695039\n",
      "0.7817026382915236\n",
      "0.7816976933416406\n",
      "0.7816933250384887\n",
      "0.7816894695405406\n",
      "0.7816860696468282\n",
      "0.781683074158347\n",
      "0.7816804372944314\n",
      "0.7816781181603429\n",
      "0.781676080262683\n",
      "0.7816742910681156\n",
      "0.7816727216025217\n",
      "0.7816713460875052\n",
      "0.7816701416104453\n",
      "0.7816690878263064\n",
      "0.7816681666873851\n",
      "0.7816673622001673\n",
      "0.7816666602049434\n",
      "0.7816660481782491\n",
      "0.781665515054318\n",
      "0.7816650510651039\n",
      "0.7816646475964114\n",
      "0.7816642970590462\n",
      "0.7816639927732789\n",
      "0.7816637288654747\n",
      "0.7816635001759736\n",
      "0.7816633021763912\n",
      "0.7816631308965808\n",
      "0.781662982858753\n",
      "0.78166285501949\n",
      "0.7816627447181785\n",
      "0.7816626496308031\n",
      "0.7816625677296225\n",
      "0.7816624972466174\n",
      "0.7816624366421913\n",
      "0.781662384576548\n",
      "0.7816623398849791\n",
      "0.7816623015560116\n",
      "0.7816622687120403\n",
      "0.7816622405921431\n",
      "0.7816622165375413\n",
      "0.7816621959780242\n",
      "0.7816621784207816\n",
      "0.7816621634401957\n",
      "0.7816621506690543\n",
      "0.78166213979072\n",
      "0.7816621305325722\n",
      "0.7816621226600148\n",
      "0.7816621159713063\n",
      "0.7816621102932952\n",
      "0.7816621054773807\n",
      "0.7816621013960653\n",
      "0.7816620979402502\n",
      "0.7816620950165751\n",
      "0.7816620925451483\n",
      "0.7816620904577851\n",
      "0.7816620886962997\n",
      "0.7816620872110608\n",
      "0.781662085959829\n",
      "0.7816620849066003\n",
      "0.7816620840207799\n",
      "0.781662083276373\n",
      "0.7816620826514102\n",
      "0.7816620821270481\n",
      "0.7816620816875153\n",
      "0.7816620813194202\n",
      "0.781662081011403\n",
      "0.7816620807538517\n",
      "0.7816620805386905\n",
      "0.7816620803590854\n",
      "0.7816620802092968\n",
      "0.7816620800844729\n",
      "0.781662079980524\n",
      "0.7816620798940542\n",
      "0.7816620798221688\n",
      "0.7816620797624559\n",
      "0.7816620797129332\n",
      "0.7816620796718474\n",
      "0.7816620796378125\n",
      "0.7816620796096417\n",
      "0.7816620795863284\n",
      "0.7816620795670778\n",
      "0.7816620795511838\n",
      "0.7816620795380472\n",
      "0.7816620795272374\n",
      "0.7816620795183234\n",
      "0.7816620795109905\n",
      "0.7816620795049657\n",
      "0.7816620795000119\n",
      "0.7816620794959348\n",
      "0.7816620794925953\n",
      "0.781662079489864\n",
      "0.7816620794876227\n",
      "0.7816620794857859\n",
      "0.7816620794842822\n",
      "0.7816620794830653\n",
      "0.78166207948206\n",
      "0.7816620794812471\n",
      "0.7816620794805823\n",
      "0.781662079480042\n",
      "0.7816620794796\n",
      "0.7816620794792443\n",
      "0.7816620794789545\n",
      "0.7816620794787223\n",
      "0.7816620794785268\n",
      "0.7816620794783647\n",
      "0.7816620794782444\n",
      "0.7816620794781407\n",
      "0.7816620794780583\n",
      "0.7816620794779938\n",
      "0.7816620794779421\n",
      "0.7816620794779029\n",
      "0.781662079477865\n",
      "0.781662079477837\n",
      "0.7816620794778136\n",
      "0.7816620794777948\n",
      "0.7816620794777777\n",
      "0.7816620794777663\n",
      "0.7816620794777568\n",
      "0.7816620794777478\n",
      "0.781662079477741\n",
      "0.7816620794777352\n",
      "0.7816620794777326\n",
      "0.7816620794777299\n",
      "0.7816620794777269\n",
      "0.7816620794777248\n",
      "0.7816620794777235\n",
      "0.7816620794777221\n",
      "0.7816620794777215\n",
      "0.7816620794777206\n",
      "0.7816620794777202\n",
      "0.7816620794777199\n",
      "0.7816620794777195\n",
      "0.7816620794777195\n",
      "0.7816620794777195\n",
      "0.7816620794777195\n",
      "0.7816620794777195\n",
      "Stopping early at iteration 266\n",
      "inf\n",
      "0.7511195082426566\n",
      "0.7343416516686637\n",
      "0.7180427253763142\n",
      "0.7022882194139558\n",
      "0.6871010507197326\n",
      "0.6724861104858117\n",
      "0.6584392681909322\n",
      "0.6449513469452915\n",
      "0.6320101424439916\n",
      "0.6196015679264629\n",
      "0.6077103614457627\n",
      "0.5963205518172371\n",
      "0.5854157804038029\n",
      "0.5749795309326733\n",
      "0.5649952974576702\n",
      "0.5554467089290214\n",
      "0.5463176222910802\n",
      "0.5375921921379853\n",
      "0.5292549225383416\n",
      "0.5212907050633673\n",
      "0.5136848459982835\n",
      "0.5064230849835871\n",
      "0.4994916068121862\n",
      "0.4928770477331058\n",
      "0.4865664973310274\n",
      "0.48054749684317155\n",
      "0.4748080346115298\n",
      "0.46933653924604535\n",
      "0.4641218709727716\n",
      "0.45915331156391315\n",
      "0.4544205531812177\n",
      "0.4499136864120851\n",
      "0.44562318773289167\n",
      "0.44153990659808684\n",
      "0.4376550523216241\n",
      "0.43396018089109606\n",
      "0.43044718183109915\n",
      "0.4271082652133992\n",
      "0.42393594889387654\n",
      "0.4209230460409637\n",
      "0.41806265300781914\n",
      "0.4153481375886191\n",
      "0.41277312768971647\n",
      "0.41033150043829497\n",
      "0.4080173717421995\n",
      "0.4058250863104895\n",
      "0.40374920813643644\n",
      "0.40178451144171967\n",
      "0.3999259720753044\n",
      "0.3981687593579978\n",
      "0.39650822836023536\n",
      "0.39493991259876504\n",
      "0.3934595171355744\n",
      "0.39206291206152427\n",
      "0.3907461263451265\n",
      "0.3895053420274339\n",
      "0.3883368887417449\n",
      "0.3872372385385279\n",
      "0.38620300099403854\n",
      "0.38523091858258274\n",
      "0.3843178622923003\n",
      "0.3834608274644376\n",
      "0.38265692983729005\n",
      "0.3819034017764091\n",
      "0.38119758867363707\n",
      "0.38053694549800887\n",
      "0.37991903348353306\n",
      "0.3793415169386442\n",
      "0.3788021601642184\n",
      "0.3782988244676129\n",
      "0.37782946526147415\n",
      "0.3773921292371562\n",
      "0.3769849516036044\n",
      "0.3766061533840085\n",
      "0.37625403876290625\n",
      "0.37592699247844286\n",
      "0.3756234772542083\n",
      "0.37534203126732546\n",
      "0.3750812656494326\n",
      "0.37483986201831004\n",
      "0.3746165700384856\n",
      "0.37441020501059036\n",
      "0.3742196454882271\n",
      "0.374043830923776\n",
      "0.3738817593430842\n",
      "0.3737324850506819\n",
      "0.37359511636679\n",
      "0.37346881339802573\n",
      "0.37335278584382736\n",
      "0.3732462908404282\n",
      "0.3731486308450015\n",
      "0.37305915156173647\n",
      "0.3729772399125539\n",
      "0.37290232205409685\n",
      "0.3728338614434569\n",
      "0.37277135695436864\n",
      "0.3727143410457815\n",
      "0.3726623779841032\n",
      "0.3726150621211284\n",
      "0.3725720162279401\n",
      "0.372532889886739\n",
      "0.37249735794058275\n",
      "0.3724651190019592\n",
      "0.37243589402040955\n",
      "0.37240942490919127\n",
      "0.37238547323083104\n",
      "0.3723638189414312\n",
      "0.372344259192874\n",
      "0.3723266071925123\n",
      "0.3723106911190302\n",
      "0.3722963530939916\n",
      "0.37228344820720427\n",
      "0.37227184359521603\n",
      "0.37226141757098286\n",
      "0.3722520588034913\n",
      "0.3722436655456603\n",
      "0.3722361449085907\n",
      "0.37222941218096584\n",
      "0.3722233901911478\n",
      "0.37221800871070343\n",
      "0.3722132038972884\n",
      "0.3722089177752205\n",
      "0.3722050977515304\n",
      "0.3722016961664045\n",
      "0.37219866987557415\n",
      "0.3721959798630743\n",
      "0.37219359088296794\n",
      "0.3721914711280414\n",
      "0.3721895919239125\n",
      "0.37218792744714163\n",
      "0.372186454465618\n",
      "0.3721851520999112\n",
      "0.37218400160419607\n",
      "0.37218298616535445\n",
      "0.37218209071904207\n",
      "0.37218130178139175\n",
      "0.3721806072954248\n",
      "0.3721799964908811\n",
      "0.37217945975661404\n",
      "0.3721789885243205\n",
      "0.37217857516315916\n",
      "0.37217821288377284\n",
      "0.37217789565167586\n",
      "0.37217761810846345\n",
      "0.37217737550095065\n",
      "0.372177163616752\n",
      "0.3721769787266037\n",
      "0.3721768175321593\n",
      "0.3721766771188938\n",
      "0.372176554914203\n",
      "0.37217644864924687\n",
      "0.37217635632525575\n",
      "0.372176276182936\n",
      "0.37217620667526713\n",
      "0.3721761464433294\n",
      "0.37217609429449144\n",
      "0.3721760491830368\n",
      "0.37217601019301527\n",
      "0.3721759765228833\n",
      "0.3721759474718534\n",
      "0.37217592242786635\n",
      "0.37217590085682833\n",
      "0.37217588229311094\n",
      "0.3721758663311847\n",
      "0.37217585261819325\n",
      "0.372175840847381\n",
      "0.37217583075234173\n",
      "0.37217582210192723\n",
      "0.37217581469575\n",
      "0.3721758083602785\n",
      "0.3721758029453194\n",
      "0.37217579832111214\n",
      "0.37217579437556725\n",
      "0.37217579101192527\n",
      "0.3721757881467904\n",
      "0.3721757857083834\n",
      "0.3721757836349208\n",
      "0.3721757818732542\n",
      "0.37217578037781185\n",
      "0.3721757791093789\n",
      "0.37217577803446283\n",
      "0.37217577712426975\n",
      "0.3721757763542267\n",
      "0.37217577570331\n",
      "0.3721757751535412\n",
      "0.3721757746895976\n",
      "0.37217577429842585\n",
      "0.37217577396888457\n",
      "0.37217577369148\n",
      "0.3721757734581883\n",
      "0.3721757732621114\n",
      "0.372175773097506\n",
      "0.3721757729593921\n",
      "0.37217577284364767\n",
      "0.3721757727466983\n",
      "0.37217577266555824\n",
      "0.3721757725977234\n",
      "0.37217577254104706\n",
      "0.37217577249375516\n",
      "0.3721757724543004\n",
      "0.37217577242142325\n",
      "0.3721757723940536\n",
      "0.3721757723712824\n",
      "0.3721757723523498\n",
      "0.3721757723366231\n",
      "0.3721757723235787\n",
      "0.3721757723127592\n",
      "0.37217577230379595\n",
      "0.37217577229637355\n",
      "0.37217577229023463\n",
      "0.37217577228516624\n",
      "0.3721757722809772\n",
      "0.37217577227751775\n",
      "0.3721757722746647\n",
      "0.37217577227231624\n",
      "0.37217577227038745\n",
      "0.37217577226879894\n",
      "0.37217577226749\n",
      "0.37217577226642035\n",
      "0.37217577226554033\n",
      "0.3721757722648234\n",
      "0.3721757722642342\n",
      "0.3721757722637496\n",
      "0.372175772263357\n",
      "0.37217577226303106\n",
      "0.3721757722627698\n",
      "0.372175772262556\n",
      "0.37217577226237925\n",
      "0.3721757722622375\n",
      "0.37217577226212106\n",
      "0.3721757722620278\n",
      "0.37217577226195064\n",
      "0.3721757722618876\n",
      "0.37217577226183723\n",
      "0.3721757722617966\n",
      "0.3721757722617643\n",
      "0.3721757722617372\n",
      "0.3721757722617159\n",
      "0.3721757722616981\n",
      "0.37217577226168497\n",
      "0.3721757722616733\n",
      "0.3721757722616637\n",
      "0.3721757722616558\n",
      "0.37217577226164933\n",
      "0.3721757722616442\n",
      "0.37217577226164006\n",
      "0.37217577226163695\n",
      "0.37217577226163423\n",
      "0.3721757722616328\n",
      "0.3721757722616309\n",
      "0.3721757722616295\n",
      "0.37217577226162823\n",
      "0.3721757722616272\n",
      "0.37217577226162674\n",
      "0.37217577226162635\n",
      "0.372175772261626\n",
      "0.37217577226162535\n",
      "0.3721757722616253\n",
      "0.3721757722616253\n",
      "0.37217577226162496\n",
      "0.3721757722616248\n",
      "0.3721757722616248\n",
      "0.3721757722616248\n",
      "0.3721757722616248\n",
      "0.3721757722616248\n",
      "Stopping early at iteration 265\n",
      "inf\n",
      "0.3652005699347408\n",
      "0.36029057274325027\n",
      "0.3554980538048738\n",
      "0.35083352067377777\n",
      "0.3463008741608023\n",
      "0.3419015338409914\n",
      "0.3376356436181989\n",
      "0.3335025819548326\n",
      "0.3295012155087978\n",
      "0.32563004072087504\n",
      "0.3218872716349669\n",
      "0.31827089973283224\n",
      "0.31477873796381234\n",
      "0.3114084550853402\n",
      "0.30815760357897176\n",
      "0.30502364299527146\n",
      "0.3020039598470531\n",
      "0.2990958847697132\n",
      "0.29629670743749864\n",
      "0.29360368958668853\n",
      "0.2910140764103329\n",
      "0.28852510653286606\n",
      "0.2861340207341786\n",
      "0.2838380695653887\n",
      "0.28163451997897837\n",
      "0.2795206610799996\n",
      "0.27749380909312754\n",
      "0.27555131163035024\n",
      "0.2736905513353878\n",
      "0.271908948973704\n",
      "0.2702039660305336\n",
      "0.2685731068731774\n",
      "0.26701392052895934\n",
      "0.2655240021246696\n",
      "0.26410099402962517\n",
      "0.26274258673950535\n",
      "0.2614465195350452\n",
      "0.2602105809454458\n",
      "0.25903260904346387\n",
      "0.2579104915963019\n",
      "0.25684216609288724\n",
      "0.25582561966678785\n",
      "0.25485888893036884\n",
      "0.2539400597348556\n",
      "0.2530672668682887\n",
      "0.25223869370186874\n",
      "0.2514525717938694\n",
      "0.25070718045831847\n",
      "0.2500008463050842\n",
      "0.24933194275630818\n",
      "0.2486988895435003\n",
      "0.24810015218857398\n",
      "0.2475342414712244\n",
      "0.24699971288494368\n",
      "0.24649516608246758\n",
      "0.24601924431189695\n",
      "0.24557063384372782\n",
      "0.2451480633890173\n",
      "0.24475030350839527\n",
      "0.24437616601163473\n",
      "0.244024503347403\n",
      "0.24369420798229885\n",
      "0.24338421176886355\n",
      "0.2430934853016124\n",
      "0.24282103726082122\n",
      "0.2425659137430476\n",
      "0.24232719757839005\n",
      "0.24210400763375498\n",
      "0.24189549810211525\n",
      "0.2417008577775853\n",
      "0.24151930931635926\n",
      "0.2413501084838051\n",
      "0.24119254338810647\n",
      "0.2410459337008399\n",
      "0.24090962986548944\n",
      "0.2407830122945322\n",
      "0.24066549055617736\n",
      "0.24055650255198155\n",
      "0.24045551368653542\n",
      "0.24036201603069673\n",
      "0.24027552747968686\n",
      "0.2401955909079172\n",
      "0.24012177332176454\n",
      "0.24005366501235476\n",
      "0.23999087870991978\n",
      "0.23993304874123503\n",
      "0.2398798301921571\n",
      "0.23983089807683136\n",
      "0.23978594651516433\n",
      "0.2397446879201748\n",
      "0.23970685219691126\n",
      "0.23967218595419004\n",
      "0.2396404517307744\n",
      "0.23961142723696557\n",
      "0.23958490461323595\n",
      "0.23956068970659486\n",
      "0.23953860136589342\n",
      "0.2395184707569128\n",
      "0.23950014069785047\n",
      "0.2394834650160803\n",
      "0.2394683079265168\n",
      "0.23945454343202546\n",
      "0.2394420547463345\n",
      "0.2394307337394589\n",
      "0.2394204804058049\n",
      "0.23941120235498237\n",
      "0.23940281432522637\n",
      "0.23939523771922513\n",
      "0.23938840016206367\n",
      "0.23938223508105663\n",
      "0.23937668130704676\n",
      "0.23937168269652098\n",
      "0.23936718777443958\n",
      "0.23936314939681438\n",
      "0.2393595244326756\n",
      "0.23935627346473756\n",
      "0.2393533605081662\n",
      "0.23935075274662834\n",
      "0.23934842028515863\n",
      "0.23934633591891685\n",
      "0.2393444749172687\n",
      "0.23934281482242376\n",
      "0.23934133526205406\n",
      "0.23934001777489244\n",
      "0.23933884564901428\n",
      "0.23933780377174269\n",
      "0.23933687849087662\n",
      "0.2393360574862942\n",
      "0.23933532965139076\n",
      "0.2393346849839488\n",
      "0.2393341144855166\n",
      "0.23933361006893342\n",
      "0.23933316447351197\n",
      "0.2393327711871661\n",
      "0.2393324243751963\n",
      "0.23933211881504093\n",
      "0.23933184983686703\n",
      "0.23933161326918018\n",
      "0.23933140538946693\n",
      "0.23933122287911682\n",
      "0.23933106278261454\n",
      "0.23933092247038854\n",
      "0.23933079960521458\n",
      "0.23933069211174693\n",
      "0.23933059814903415\n",
      "0.23933051608560796\n",
      "0.23933044447709517\n",
      "0.23933038204599485\n",
      "0.23933032766354323\n",
      "0.2393302803333153\n",
      "0.23933023917662047\n",
      "0.23933020341929595\n",
      "0.23933017237996324\n",
      "0.23933014545947506\n",
      "0.2393301221315117\n",
      "0.2393301019341315\n",
      "0.23933008446235762\n",
      "0.23933006936143808\n",
      "0.23933005632091334\n",
      "0.23933004506938535\n",
      "0.23933003536980263\n",
      "0.2393300270152805\n",
      "0.23933001982550237\n",
      "0.23933001364342293\n",
      "0.23933000833234616\n",
      "0.23933000377348668\n",
      "0.23932999986366216\n",
      "0.23932999651334025\n",
      "0.23932999364490068\n",
      "0.23932999119115914\n",
      "0.239329989093943\n",
      "0.2393299873029739\n",
      "0.23932998577485565\n",
      "0.23932998447211018\n",
      "0.2393299833624332\n",
      "0.23932998241804068\n",
      "0.23932998161498692\n",
      "0.23932998093268715\n",
      "0.23932998035349265\n",
      "0.23932997986222898\n",
      "0.239329979445912\n",
      "0.2393299790933852\n",
      "0.23932997879515275\n",
      "0.23932997854305008\n",
      "0.23932997833012906\n",
      "0.2393299781504332\n",
      "0.23932997799893369\n",
      "0.2393299778712969\n",
      "0.23932997776386153\n",
      "0.2393299776735024\n",
      "0.23932997759756663\n",
      "0.23932997753380722\n",
      "0.23932997748032983\n",
      "0.23932997743549123\n",
      "0.23932997739794498\n",
      "0.23932997736652542\n",
      "0.2393299773402506\n",
      "0.2393299773183048\n",
      "0.23932997729998184\n",
      "0.23932997728470204\n",
      "0.23932997727197164\n",
      "0.2393299772613723\n",
      "0.2393299772525538\n",
      "0.23932997724522134\n",
      "0.23932997723913027\n",
      "0.23932997723407706\n",
      "0.23932997722988733\n",
      "0.2393299772264147\n",
      "0.239329977223539\n",
      "0.23932997722115915\n",
      "0.23932997721919458\n",
      "0.239329977217571\n",
      "0.2393299772162339\n",
      "0.23932997721513127\n",
      "0.23932997721422056\n",
      "0.2393299772134731\n",
      "0.23932997721285604\n",
      "0.23932997721235263\n",
      "0.2393299772119361\n",
      "0.2393299772115978\n",
      "0.23932997721131619\n",
      "0.23932997721108548\n",
      "0.23932997721090088\n",
      "0.2393299772107492\n",
      "0.239329977210623\n",
      "0.23932997721052113\n",
      "0.23932997721043747\n",
      "0.2393299772103694\n",
      "0.23932997721031385\n",
      "0.23932997721026844\n",
      "0.23932997721023105\n",
      "0.23932997721020005\n",
      "0.23932997721017688\n",
      "0.23932997721015845\n",
      "0.23932997721014201\n",
      "0.23932997721012914\n",
      "0.23932997721011956\n",
      "0.2393299772101113\n",
      "0.2393299772101048\n",
      "0.23932997721009858\n",
      "0.23932997721009394\n",
      "0.23932997721009075\n",
      "0.23932997721008745\n",
      "0.2393299772100847\n",
      "0.23932997721008226\n",
      "0.23932997721008037\n",
      "0.23932997721007881\n",
      "0.23932997721007768\n",
      "0.23932997721007684\n",
      "0.23932997721007596\n",
      "0.23932997721007548\n",
      "0.23932997721007512\n",
      "0.23932997721007468\n",
      "0.23932997721007424\n",
      "0.2393299772100741\n",
      "0.23932997721007396\n",
      "0.2393299772100736\n",
      "0.23932997721007354\n",
      "0.23932997721007354\n",
      "0.23932997721007349\n",
      "0.23932997721007349\n",
      "0.23932997721007343\n",
      "0.23932997721007335\n",
      "0.23932997721007335\n",
      "0.23932997721007335\n",
      "0.23932997721007335\n",
      "0.23932997721007335\n",
      "Stopping early at iteration 267\n",
      "inf\n",
      "0.23359023609744484\n",
      "0.2313493020390166\n",
      "0.229239906245432\n",
      "0.2272310422566958\n",
      "0.22530568842760795\n",
      "0.22345336689786416\n",
      "0.2216671487128335\n",
      "0.21994220407411533\n",
      "0.2182750147424826\n",
      "0.21666291310569166\n",
      "0.21510379786832232\n",
      "0.21359595159563324\n",
      "0.2121379199634719\n",
      "0.210728429911137\n",
      "0.2093663331463884\n",
      "0.2080505666364412\n",
      "0.20678012474746166\n",
      "0.2055540395283102\n",
      "0.2043713667805274\n",
      "0.2032311762922796\n",
      "0.20213254509964268\n",
      "0.20107455296526597\n",
      "0.2000562794892067\n",
      "0.19907680242382997\n",
      "0.1981351968762998\n",
      "0.19723053516279207\n",
      "0.1963618871369817\n",
      "0.1955283208590409\n",
      "0.19472890350323172\n",
      "0.1939627024267871\n",
      "0.19322878634099291\n",
      "0.19252622653930423\n",
      "0.19185409814830526\n",
      "0.19121148137538047\n",
      "0.19059746273364395\n",
      "0.1900111362292614\n",
      "0.18945160450071172\n",
      "0.18891797990211961\n",
      "0.18840938552535694\n",
      "0.18792495615750981\n",
      "0.18746383917161402\n",
      "0.1870251953498235\n",
      "0.18660819963888617\n",
      "0.18621204183868278\n",
      "0.18583592722491696\n",
      "0.18547907710756087\n",
      "0.18514072932675057\n",
      "0.18482013868841918\n",
      "0.18451657734162208\n",
      "0.1842293350999449\n",
      "0.1839577197093693\n",
      "0.18370105706486764\n",
      "0.18345869137823934\n",
      "0.18322998529933543\n",
      "0.18301431999327553\n",
      "0.18281109517574295\n",
      "0.18261972910855492\n",
      "0.18243965855803101\n",
      "0.18227033871790396\n",
      "0.18211124309909987\n",
      "0.1819618633883377\n",
      "0.18182170927763366\n",
      "0.18169030826636187\n",
      "0.18156720543816335\n",
      "0.18145196321404775\n",
      "0.1813441610839353\n",
      "0.18124339531793068\n",
      "0.18114927865935043\n",
      "0.18106144000089547\n",
      "0.18097952404578102\n",
      "0.18090319095509463\n",
      "0.18083211598315888\n",
      "0.18076598910227232\n",
      "0.18070451461810122\n",
      "0.18064741077739188\n",
      "0.1805944093690104\n",
      "0.18054525531994925\n",
      "0.18049970628725748\n",
      "0.18045753224720731\n",
      "0.1804185150829572\n",
      "0.1803824481716197\n",
      "0.18034913597202176\n",
      "0.1803183936138908\n",
      "0.18029004648973526\n",
      "0.18026392985008127\n",
      "0.18023988840304989\n",
      "0.18021777591898297\n",
      "0.18019745484101315\n",
      "0.18017879590205305\n",
      "0.18016167774907782\n",
      "0.18014598657500058\n",
      "0.1801316157590069\n",
      "0.18011846551542005\n",
      "0.18010644255198843\n",
      "0.18009545973738667\n",
      "0.1800854357787775\n",
      "0.1800762949093829\n",
      "0.1800679665863091\n",
      "0.18006038519885878\n",
      "0.18005348978748512\n",
      "0.1800472237732202\n",
      "0.1800415346979585\n",
      "0.18003637397524105\n",
      "0.18003169665181062\n",
      "0.18002746117951024\n",
      "0.18002362919773732\n",
      "0.1800201653261396\n",
      "0.18001703696729957\n",
      "0.18001421411948526\n",
      "0.18001166919894113\n",
      "0.18000937687159088\n",
      "0.18000731389407956\n",
      "0.18000545896354753\n",
      "0.1800037925761403\n",
      "0.18000229689375472\n",
      "0.180000955618839\n",
      "0.17999975387692113\n",
      "0.1799986781065158\n",
      "0.17999771595613692\n",
      "0.17999685618799485\n",
      "0.17999608858822116\n",
      "0.1799954038832355\n",
      "0.17999479366186324\n",
      "0.17999425030301422\n",
      "0.17999376690863983\n",
      "0.179993337241512\n",
      "0.1799929556677321\n",
      "0.1799926171036206\n",
      "0.17999231696657936\n",
      "0.1799920511300444\n",
      "0.17999181588179214\n",
      "0.17999160788576932\n",
      "0.17999142414703523\n",
      "0.17999126197951545\n",
      "0.17999111897671047\n",
      "0.17999099298466797\n",
      "0.17999088207743338\n",
      "0.17999078453468517\n",
      "0.17999069882125251\n",
      "0.17999062356862286\n",
      "0.17999055755801155\n",
      "0.1799904997050771\n",
      "0.17999044904604158\n",
      "0.1799904047251394\n",
      "0.17999036598327353\n",
      "0.1799903321476973\n",
      "0.17999030262286667\n",
      "0.17999027688201652\n",
      "0.1799902544597439\n",
      "0.17999023494518335\n",
      "0.17999021797603879\n",
      "0.1799902032331037\n",
      "0.17999019043544262\n",
      "0.1799901793360024\n",
      "0.17999016971778226\n",
      "0.17999016139033766\n",
      "0.17999015418666103\n",
      "0.17999014796050233\n",
      "0.17999014258385418\n",
      "0.17999013794482085\n",
      "0.17999013394565108\n",
      "0.17999013050106533\n",
      "0.17999012753670926\n",
      "0.17999012498781827\n",
      "0.17999012279806045\n",
      "0.17999012091842367\n",
      "0.17999011930639483\n",
      "0.1799901179250524\n",
      "0.17999011674239346\n",
      "0.1799901157307058\n",
      "0.17999011486602318\n",
      "0.17999011412761204\n",
      "0.17999011349755398\n",
      "0.17999011296043357\n",
      "0.17999011250291122\n",
      "0.1799901121135357\n",
      "0.17999011178242605\n",
      "0.17999011150112693\n",
      "0.17999011126231587\n",
      "0.17999011105976365\n",
      "0.17999011088811576\n",
      "0.17999011074277366\n",
      "0.17999011061982048\n",
      "0.17999011051586533\n",
      "0.1799901104280864\n",
      "0.17999011035399812\n",
      "0.17999011029152953\n",
      "0.17999011023890513\n",
      "0.1799901101946102\n",
      "0.17999011015735705\n",
      "0.1799901101260527\n",
      "0.17999011009976523\n",
      "0.1799901100777104\n",
      "0.17999011005922813\n",
      "0.1799901100437461\n",
      "0.179990110030792\n",
      "0.1799901100199618\n",
      "0.17999011001091123\n",
      "0.17999011000335477\n",
      "0.17999010999705492\n",
      "0.1799901099918072\n",
      "0.17999010998743403\n",
      "0.17999010998379805\n",
      "0.1799901099807762\n",
      "0.1799901099782648\n",
      "0.17999010997618028\n",
      "0.1799901099744556\n",
      "0.17999010997302547\n",
      "0.1799901099718396\n",
      "0.17999010997085854\n",
      "0.17999010997004827\n",
      "0.17999010996937959\n",
      "0.17999010996882825\n",
      "0.17999010996837345\n",
      "0.17999010996799766\n",
      "0.179990109967691\n",
      "0.1799901099674357\n",
      "0.17999010996722817\n",
      "0.17999010996705797\n",
      "0.17999010996691917\n",
      "0.179990109966804\n",
      "0.17999010996671\n",
      "0.17999010996663223\n",
      "0.17999010996656903\n",
      "0.17999010996651904\n",
      "0.17999010996647563\n",
      "0.17999010996644066\n",
      "0.17999010996641368\n",
      "0.17999010996639048\n",
      "0.17999010996637127\n",
      "0.1799901099663573\n",
      "0.17999010996634562\n",
      "0.17999010996633583\n",
      "0.17999010996632772\n",
      "0.17999010996632125\n",
      "0.17999010996631581\n",
      "0.1799901099663112\n",
      "0.17999010996630826\n",
      "0.17999010996630557\n",
      "0.17999010996630338\n",
      "0.1799901099663011\n",
      "0.1799901099662996\n",
      "0.1799901099662981\n",
      "0.17999010996629689\n",
      "0.17999010996629586\n",
      "0.1799901099662952\n",
      "0.17999010996629455\n",
      "0.179990109966294\n",
      "0.17999010996629367\n",
      "0.17999010996629317\n",
      "0.1799901099662929\n",
      "0.17999010996629264\n",
      "0.17999010996629242\n",
      "0.17999010996629214\n",
      "0.17999010996629192\n",
      "0.17999010996629186\n",
      "0.1799901099662917\n",
      "0.1799901099662917\n",
      "0.1799901099662917\n",
      "0.1799901099662916\n",
      "0.1799901099662916\n",
      "0.1799901099662916\n",
      "0.1799901099662916\n",
      "0.1799901099662916\n",
      "Stopping early at iteration 264\n",
      "inf\n",
      "0.17797005438965446\n",
      "0.176339961034316\n",
      "0.1749057388428398\n",
      "0.17360417224216734\n",
      "0.17240129270954993\n",
      "0.17127634585346121\n",
      "0.17021568794119463\n",
      "0.16920989942026005\n",
      "0.1682522371561777\n",
      "0.16733773081773912\n",
      "0.16646262236698403\n",
      "0.16562400255086016\n",
      "0.1648195670968262\n",
      "0.1640474488830055\n",
      "0.16330610001123502\n",
      "0.162594207576248\n",
      "0.16191063271115042\n",
      "0.161254366020518\n",
      "0.16062449473853632\n",
      "0.16002017839286906\n",
      "0.15944063071211056\n",
      "0.15888510616252965\n",
      "0.1583528899469645\n",
      "0.1578432906109261\n",
      "0.15735563462349356\n",
      "0.1568892624599519\n",
      "0.15644352582969723\n",
      "0.1560177857781881\n",
      "0.15561141145530225\n",
      "0.1552237793901414\n",
      "0.154854273148325\n",
      "0.1545022832751024\n",
      "0.15416720744892984\n",
      "0.15384845078636009\n",
      "0.1535454262516529\n",
      "0.15325755513472553\n",
      "0.15298426756878955\n",
      "0.15272500306496922\n",
      "0.15247921104674272\n",
      "0.15224635137034295\n",
      "0.15202589482085274\n",
      "0.15181732357625585\n",
      "0.151620131633517\n",
      "0.15143382519275664\n",
      "0.1512579229967314\n",
      "0.15109195662415867\n",
      "0.15093547073594446\n",
      "0.15078802327449822\n",
      "0.15064918561681925\n",
      "0.15051854268224574\n",
      "0.15039569299643074\n",
      "0.15028024871335754\n",
      "0.15017183559740271\n",
      "0.15007009296731336\n",
      "0.14997467360500352\n",
      "0.1498852436309084\n",
      "0.14980148234882038\n",
      "0.14972308206244156\n",
      "0.14964974786621424\n",
      "0.14958119741291998\n",
      "0.1495171606603558\n",
      "0.14945737959958594\n",
      "0.14940160796703947\n",
      "0.14934961094265767\n",
      "0.14930116483626155\n",
      "0.14925605676420237\n",
      "0.1492140843182384\n",
      "0.14917505522851982\n",
      "0.14913878702235203\n",
      "0.14910510668048899\n",
      "0.14907385029232495\n",
      "0.14904486271152437\n",
      "0.14901799721335832\n",
      "0.1489931151547565\n",
      "0.14897008563843278\n",
      "0.14894878518180796\n",
      "0.14892909739163332\n",
      "0.14891091264513467\n",
      "0.14889412777822372\n",
      "0.14887864578146887\n",
      "0.14886437550405948\n",
      "0.1488512313664127\n",
      "0.14883913308156807\n",
      "0.14882800538559865\n",
      "0.14881777777727037\n",
      "0.1488083842670063\n",
      "0.14879976313520252\n",
      "0.14879185669994316\n",
      "0.14878461109395788\n",
      "0.1487779760508762\n",
      "0.1487719047005161\n",
      "0.14876635337323812\n",
      "0.14876128141291087\n",
      "0.1487566509985889\n",
      "0.14875242697451607\n",
      "0.14874857668828423\n",
      "0.14874506983686825\n",
      "0.14874187832031266\n",
      "0.14873897610288592\n",
      "0.14873633908128295\n",
      "0.14873394495984177\n",
      "0.14873177313230443\n",
      "0.14872980456998683\n",
      "0.14872802171612676\n",
      "0.14872640838607704\n",
      "0.14872494967311659\n",
      "0.1487236318597036\n",
      "0.1487224423338764\n",
      "0.14872136951058737\n",
      "0.14872040275775475\n",
      "0.14871953232683302\n",
      "0.1487187492876058\n",
      "0.1487180454671122\n",
      "0.14871741339243116\n",
      "0.14871684623720177\n",
      "0.14871633777150928\n",
      "0.1487158823152797\n",
      "0.14871547469465482\n",
      "0.1487151102014605\n",
      "0.14871478455537127\n",
      "0.14871449386887883\n",
      "0.14871423461461797\n",
      "0.14871400359518042\n",
      "0.14871379791505504\n",
      "0.14871361495472177\n",
      "0.14871345234664252\n",
      "0.1487133079531859\n",
      "0.14871317984612933\n",
      "0.14871306628795122\n",
      "0.14871296571444692\n",
      "0.14871287671886044\n",
      "0.1487127980373051\n",
      "0.1487127285353378\n",
      "0.14871266719576043\n",
      "0.14871261310735842\n",
      "0.14871256545468756\n",
      "0.14871252350871203\n",
      "0.14871248661834324\n",
      "0.14871245420261703\n",
      "0.14871242574366847\n",
      "0.14871240078034104\n",
      "0.14871237890241207\n",
      "0.1487123597452632\n",
      "0.14871234298516542\n",
      "0.14871232833497167\n",
      "0.14871231554023462\n",
      "0.14871230437568542\n",
      "0.14871229464210622\n",
      "0.14871228616345608\n",
      "0.14871227878437432\n",
      "0.1487122723678235\n",
      "0.14871226679310282\n",
      "0.14871226195396908\n",
      "0.14871225775700017\n",
      "0.1487122541201265\n",
      "0.14871225097132137\n",
      "0.14871224824745716\n",
      "0.1487122458932136\n",
      "0.14871224386019957\n",
      "0.14871224210608994\n",
      "0.1487122405939347\n",
      "0.148712239291472\n",
      "0.14871223817058998\n",
      "0.14871223720681562\n",
      "0.14871223637882902\n",
      "0.14871223566811814\n",
      "0.14871223505857292\n",
      "0.14871223453626398\n",
      "0.1487122340890853\n",
      "0.148712233706544\n",
      "0.14871223337959147\n",
      "0.14871223310038195\n",
      "0.14871223286214813\n",
      "0.14871223265906047\n",
      "0.14871223248607218\n",
      "0.1487122323388384\n",
      "0.1487122322136435\n",
      "0.14871223210726842\n",
      "0.14871223201698008\n",
      "0.14871223194038782\n",
      "0.14871223187549257\n",
      "0.14871223182053225\n",
      "0.14871223177403337\n",
      "0.14871223173473358\n",
      "0.14871223170154338\n",
      "0.14871223167352832\n",
      "0.14871223164990824\n",
      "0.14871223163001845\n",
      "0.1487122316132681\n",
      "0.14871223159917363\n",
      "0.14871223158733682\n",
      "0.14871223157739988\n",
      "0.1487122315690633\n",
      "0.14871223156207072\n",
      "0.14871223155621918\n",
      "0.14871223155131497\n",
      "0.14871223154721944\n",
      "0.14871223154380006\n",
      "0.1487122315409412\n",
      "0.14871223153855687\n",
      "0.14871223153657256\n",
      "0.14871223153492263\n",
      "0.14871223153354565\n",
      "0.14871223153240057\n",
      "0.148712231531452\n",
      "0.14871223153066393\n",
      "0.14871223153001167\n",
      "0.1487122315294691\n",
      "0.14871223152901888\n",
      "0.14871223152864932\n",
      "0.14871223152833893\n",
      "0.14871223152808624\n",
      "0.14871223152787758\n",
      "0.1487122315277084\n",
      "0.14871223152756632\n",
      "0.14871223152744942\n",
      "0.14871223152735327\n",
      "0.14871223152727495\n",
      "0.14871223152721114\n",
      "0.1487122315271575\n",
      "0.14871223152711427\n",
      "0.14871223152707835\n",
      "0.14871223152704854\n",
      "0.14871223152702434\n",
      "0.14871223152700583\n",
      "0.14871223152698876\n",
      "0.14871223152697616\n",
      "0.14871223152696522\n",
      "0.14871223152695676\n",
      "0.1487122315269498\n",
      "0.14871223152694477\n",
      "0.14871223152694052\n",
      "0.1487122315269369\n",
      "0.14871223152693375\n",
      "0.14871223152693147\n",
      "0.14871223152692942\n",
      "0.14871223152692786\n",
      "0.14871223152692664\n",
      "0.14871223152692545\n",
      "0.14871223152692462\n",
      "0.14871223152692384\n",
      "0.1487122315269232\n",
      "0.14871223152692245\n",
      "0.14871223152692184\n",
      "0.14871223152692112\n",
      "0.14871223152692076\n",
      "0.1487122315269206\n",
      "0.14871223152692006\n",
      "0.14871223152691973\n",
      "0.14871223152691943\n",
      "0.14871223152691918\n",
      "0.14871223152691906\n",
      "0.14871223152691887\n",
      "0.14871223152691876\n",
      "0.14871223152691862\n",
      "0.14871223152691831\n",
      "0.1487122315269183\n",
      "0.14871223152691823\n",
      "0.14871223152691818\n",
      "0.14871223152691818\n",
      "0.14871223152691818\n",
      "0.14871223152691818\n",
      "0.14871223152691818\n",
      "Stopping early at iteration 263\n",
      "Best number of features: 25 with average validation loss: 0.3443740340885255\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_and_evaluate() got an unexpected keyword argument 'validation_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[395], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest number of features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_num_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with average validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_avg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Retrain the model with the best number of features on the entire training set\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# and evaluate on the separate validation set\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m final_validation_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_num_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhours_transformed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhoursTrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Replace with your entire training data\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhours_transformed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhoursValid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Replace with your actual validation data\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal validation loss with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_num_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_validation_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: train_and_evaluate() got an unexpected keyword argument 'validation_data'"
     ]
    }
   ],
   "source": [
    "#cross valid\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def cross_validate(num_features, lamb, learning_rate, iterations, tolerance, early_stopping_rounds, train_data):\n",
    "    kf = KFold(n_splits=5)  # 5-fold cross-validation\n",
    "    fold_validation_losses = []\n",
    "\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        train_subset = [train_data[i] for i in train_index]\n",
    "        validation_subset = [train_data[i] for i in test_index]\n",
    "\n",
    "        # Initialize the latent factors with the new number of features\n",
    "        alpha = sum([r for _, _, r in train_subset]) / len(train_subset)\n",
    "        betaU = {u: random.random() * 0.1 for u, _, _ in train_subset}\n",
    "        betaI = {g: random.random() * 0.1 for _, g, _ in train_subset}\n",
    "        gamma_u = {u: [random.random() / sqrt(num_features) for _ in range(num_features)] for u, _, _ in train_subset}\n",
    "        gamma_i = {g: [random.random() / sqrt(num_features) for _ in range(num_features)] for _, g, _ in train_subset}\n",
    "    \n",
    "        # Train the model\n",
    "        best_betaU, best_betaI, best_gamma_u, best_gamma_i, best_loss = train(\n",
    "            lamb=lamb, \n",
    "            iterations=iterations, \n",
    "            initial_learning_rate=learning_rate, \n",
    "            tolerance=tolerance, \n",
    "            early_stopping_rounds=early_stopping_rounds, \n",
    "            validation_data=validation_subset\n",
    "        )\n",
    "    \n",
    "        # Calculate the loss on the validation subset\n",
    "        validation_loss = calculate_validation_loss(validation_subset, lamb)\n",
    "        fold_validation_losses.append(validation_loss)\n",
    "\n",
    "    average_validation_loss = np.mean(fold_validation_losses)\n",
    "    return average_validation_loss\n",
    "\n",
    "# Define a range of feature counts to try\n",
    "feature_counts = [25]\n",
    "average_validation_losses = []\n",
    "\n",
    "# Perform cross-validation on the training set\n",
    "for num_features in feature_counts:\n",
    "    print(f\"Evaluating model with {num_features} features...\")\n",
    "    avg_loss = cross_validate(\n",
    "        num_features=num_features,\n",
    "        lamb=0.01,\n",
    "        learning_rate=0.003,\n",
    "        iterations=1000,\n",
    "        tolerance=1e-5,\n",
    "        early_stopping_rounds=5,\n",
    "        train_data=[(u, g, d['hours_transformed']) for u, g, d in hoursTrain]  # Replace with your actual training data\n",
    "    )\n",
    "    average_validation_losses.append((num_features, avg_loss))\n",
    "\n",
    "# Select the number of features with the lowest average validation loss\n",
    "best_num_features, best_avg_loss = min(average_validation_losses, key=lambda x: x[1])\n",
    "print(f'Best number of features: {best_num_features} with average validation loss: {best_avg_loss}')\n",
    "\n",
    "# Retrain the model with the best number of features on the entire training set\n",
    "# and evaluate on the separate validation set\n",
    "final_validation_loss = train_and_evaluate(\n",
    "    num_features=best_num_features,\n",
    "    lamb=0.01,\n",
    "    learning_rate=0.003,\n",
    "    iterations=1000,\n",
    "    tolerance=1e-5,\n",
    "    early_stopping_rounds=5,\n",
    "    train_data=[(u, g, d['hours_transformed']) for u, g, d in hoursTrain],  # Replace with your entire training data\n",
    "    validation_data=[(u, g, d['hours_transformed']) for u, g, d in hoursValid]  # Replace with your actual validation data\n",
    ")\n",
    "\n",
    "print(f'Final validation loss with {best_num_features} features: {final_validation_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "08ca126c-17e5-46d6-bdfe-46d9aacd4768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.10839751996922"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [predict(u,g) for u,g,d in hoursValid]\n",
    "y = [d['hours_transformed'] for u,g,d in hoursValid]\n",
    "\n",
    "mse = mean_squared_error(predictions, y)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "7c8984d2-363d-4d5d-add3-705ed29540d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 5.108397519969221\n"
     ]
    }
   ],
   "source": [
    "def predictcross(u, g, alpha, betaU, betaI, gamma_u, gamma_i):\n",
    "    \"\"\"Predicts the hours played using the model parameters.\"\"\"\n",
    "    user_factor = gamma_u.get(u, [0]*len(gamma_u[list(gamma_u.keys())[0]]))\n",
    "    item_factor = gamma_i.get(g, [0]*len(gamma_i[list(gamma_i.keys())[0]]))\n",
    "    dot_product = sum(user_factor[k] * item_factor[k] for k in range(len(user_factor)))\n",
    "    return alpha + betaU.get(u, 0) + betaI.get(g, 0) + dot_product\n",
    "\n",
    "def calculate_mse(validation_data, alpha, betaU, betaI, gamma_u, gamma_i):\n",
    "    \"\"\"Calculates the mean squared error on the validation set.\"\"\"\n",
    "    errors = []\n",
    "    for u, g, actual_hours in validation_data:\n",
    "        predicted_hours = predictcross(u, g, alpha, betaU, betaI, gamma_u, gamma_i)\n",
    "        errors.append((predicted_hours - actual_hours) ** 2)\n",
    "    mse = sum(errors) / len(errors)\n",
    "    return mse\n",
    "\n",
    "\n",
    "mse = calculate_mse([(u, g, d['hours_transformed']) for u, g, d in hoursValid], alpha, betaU, betaI, gamma_u, gamma_i)\n",
    "print(\"MSE on validation set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "85c314cc-8152-41c3-9653-2db7313de5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('u70666506',\n",
       " 'g49368897',\n",
       " {'userID': 'u70666506',\n",
       "  'early_access': False,\n",
       "  'hours': 63.5,\n",
       "  'hours_transformed': 6.011227255423254,\n",
       "  'found_funny': 1,\n",
       "  'text': 'If you want to sit in queue for 10-20min and have 140 ping then this game is perfect for you :)',\n",
       "  'gameID': 'g49368897',\n",
       "  'user_id': '76561198030408772',\n",
       "  'date': '2017-05-20'})"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoursTrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b5f01e0f-11e6-4688-9176-a4a1bfccda6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.999045867911261"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betaU, betaI, mse2 = iterate(2)\n",
    "mse2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c5f7aaef-c18f-4f66-8d20-a42519f7fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Hours.csv\", 'w')\n",
    "for l in open(\"/Users/zhiqiaogong/Projects/JupyterNotebook/cse258/hw3/pairs_Hours.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    \n",
    "    re = predict(u,g)\n",
    "    \n",
    "    _ = predictions.write(u + ',' + g + ',' + str(re) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae174441-3c7e-4b41-8869-7a67b6c61607",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = globalAverage # Could initialize anywhere, this is a guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ced4bf80-22e7-44eb-9efe-e2ec42f893bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(lamb):\n",
    "    newAlpha = 0\n",
    "    for u,g,d in hoursTrain:\n",
    "        r = d['hours_transformed']\n",
    "        newAlpha += r - (betaU[u] + betaI[g])\n",
    "    alpha = newAlpha / len(hoursTrain)\n",
    "    for u in hoursPerUser:\n",
    "        newBetaU = 0\n",
    "        for g,r in hoursPerUser[u]:\n",
    "            newBetaU += r - (alpha + betaI[g])\n",
    "        betaU[u] = newBetaU / (lamb + len(hoursPerUser[u]))\n",
    "    for g in hoursPerItem:\n",
    "        newBetaI = 0\n",
    "        for u,r in hoursPerItem[g]:\n",
    "            newBetaI += r - (alpha + betaU[u])\n",
    "        betaI[g] = newBetaI / (lamb + len(hoursPerItem[g]))\n",
    "    mse = 0\n",
    "    for u,g,d in hoursTrain:\n",
    "        r = d['hours_transformed']\n",
    "        prediction = alpha + betaU[u] + betaI[g]\n",
    "        mse += (r - prediction)**2\n",
    "    regularizer = 0\n",
    "    for u in betaU:\n",
    "        regularizer += betaU[u]**2\n",
    "    for g in betaI:\n",
    "        regularizer += betaI[g]**2\n",
    "    mse /= len(hoursTrain)\n",
    "    return mse, mse + lamb*regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "819985c3-55d2-40f8-9134-ca6cd456e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse,objective = iterate(1)\n",
    "newMSE,newObjective = iterate(1)\n",
    "iterations = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2433a9ba-c08f-438b-898b-37433c4f52ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective after 3 iterations = 6916.291258826528\n",
      "MSE after 3 iterations = 2.756414053005335\n",
      "Objective after 4 iterations = 6935.23715550776\n",
      "MSE after 4 iterations = 2.755604333875777\n",
      "Objective after 5 iterations = 6924.6062017768245\n",
      "MSE after 5 iterations = 2.755486661616215\n",
      "Objective after 6 iterations = 6905.833993834695\n",
      "MSE after 6 iterations = 2.755457716152593\n",
      "Objective after 7 iterations = 6885.30918519728\n",
      "MSE after 7 iterations = 2.7554456693587497\n",
      "Objective after 8 iterations = 6864.742698779786\n",
      "MSE after 8 iterations = 2.7554377123048535\n",
      "Objective after 9 iterations = 6844.576219662253\n",
      "MSE after 9 iterations = 2.755430906995692\n",
      "Objective after 10 iterations = 6824.91582215507\n",
      "MSE after 10 iterations = 2.7554245073468486\n",
      "Objective after 11 iterations = 6805.779255606049\n",
      "MSE after 11 iterations = 2.7554183159504917\n",
      "Objective after 12 iterations = 6787.161156344411\n",
      "MSE after 12 iterations = 2.755412278370264\n",
      "Objective after 13 iterations = 6769.050270855048\n",
      "MSE after 13 iterations = 2.7554063779432876\n",
      "Objective after 14 iterations = 6751.43401216254\n",
      "MSE after 14 iterations = 2.7554006080674713\n",
      "Objective after 15 iterations = 6734.299664474671\n",
      "MSE after 15 iterations = 2.7553949648590272\n",
      "Objective after 16 iterations = 6717.634699118593\n",
      "MSE after 16 iterations = 2.7553894452023\n",
      "Objective after 17 iterations = 6701.42685449733\n",
      "MSE after 17 iterations = 2.7553840462308505\n",
      "Objective after 18 iterations = 6685.664153329271\n",
      "MSE after 18 iterations = 2.75537876518856\n",
      "Objective after 19 iterations = 6670.334903216617\n",
      "MSE after 19 iterations = 2.7553735993920143\n",
      "Objective after 20 iterations = 6655.427692791793\n",
      "MSE after 20 iterations = 2.755368546219614\n",
      "Objective after 21 iterations = 6640.931386705938\n",
      "MSE after 21 iterations = 2.755363603107894\n",
      "Objective after 22 iterations = 6626.835120331246\n",
      "MSE after 22 iterations = 2.7553587675502897\n",
      "Objective after 23 iterations = 6613.128294413673\n",
      "MSE after 23 iterations = 2.75535403709532\n",
      "Objective after 24 iterations = 6599.800569742033\n",
      "MSE after 24 iterations = 2.755349409346207\n",
      "Objective after 25 iterations = 6586.841861854152\n",
      "MSE after 25 iterations = 2.7553448819592377\n",
      "Objective after 26 iterations = 6574.242335788143\n",
      "MSE after 26 iterations = 2.7553404526430616\n",
      "Objective after 27 iterations = 6561.992400884105\n",
      "MSE after 27 iterations = 2.7553361191579246\n",
      "Objective after 28 iterations = 6550.0827056394655\n",
      "MSE after 28 iterations = 2.7553318793142894\n",
      "Objective after 29 iterations = 6538.504132621169\n",
      "MSE after 29 iterations = 2.7553277309720334\n",
      "Objective after 30 iterations = 6527.247793437296\n",
      "MSE after 30 iterations = 2.7553236720397396\n",
      "Objective after 31 iterations = 6516.30502377073\n",
      "MSE after 31 iterations = 2.7553197004729713\n",
      "Objective after 32 iterations = 6505.667378476852\n",
      "MSE after 32 iterations = 2.7553158142743737\n",
      "Objective after 33 iterations = 6495.326626747495\n",
      "MSE after 33 iterations = 2.7553120114919576\n",
      "Objective after 34 iterations = 6485.274747342254\n",
      "MSE after 34 iterations = 2.755308290218471\n",
      "Objective after 35 iterations = 6475.503923889192\n",
      "MSE after 35 iterations = 2.7553046485902595\n",
      "Objective after 36 iterations = 6466.006540256186\n",
      "MSE after 36 iterations = 2.755301084786918\n",
      "Objective after 37 iterations = 6456.775175993412\n",
      "MSE after 37 iterations = 2.7552975970296427\n",
      "Objective after 38 iterations = 6447.8026018486835\n",
      "MSE after 38 iterations = 2.755294183580897\n",
      "Objective after 39 iterations = 6439.081775355684\n",
      "MSE after 39 iterations = 2.755290842743317\n",
      "Objective after 40 iterations = 6430.605836496414\n",
      "MSE after 40 iterations = 2.7552875728588435\n",
      "Objective after 41 iterations = 6422.36810343727\n",
      "MSE after 41 iterations = 2.755284372308089\n",
      "Objective after 42 iterations = 6414.3620683403\n",
      "MSE after 42 iterations = 2.755281239509146\n",
      "Objective after 43 iterations = 6406.5813932488045\n",
      "MSE after 43 iterations = 2.7552781729172677\n",
      "Objective after 44 iterations = 6399.019906047929\n",
      "MSE after 44 iterations = 2.755275171023384\n",
      "Objective after 45 iterations = 6391.671596499879\n",
      "MSE after 45 iterations = 2.7552722323539034\n",
      "Objective after 46 iterations = 6384.530612354044\n",
      "MSE after 46 iterations = 2.7552693554698893\n",
      "Objective after 47 iterations = 6377.591255531202\n",
      "MSE after 47 iterations = 2.755266538965634\n",
      "Objective after 48 iterations = 6370.847978382115\n",
      "MSE after 48 iterations = 2.755263781468952\n",
      "Objective after 49 iterations = 6364.295380019602\n",
      "MSE after 49 iterations = 2.7552610816395213\n",
      "Objective after 50 iterations = 6357.928202724139\n",
      "MSE after 50 iterations = 2.7552584381686684\n",
      "Objective after 51 iterations = 6351.74132842174\n",
      "MSE after 51 iterations = 2.7552558497783872\n",
      "Objective after 52 iterations = 6345.729775234503\n",
      "MSE after 52 iterations = 2.7552533152208745\n",
      "Objective after 53 iterations = 6339.888694102179\n",
      "MSE after 53 iterations = 2.7552508332777172\n",
      "Objective after 54 iterations = 6334.213365474639\n",
      "MSE after 54 iterations = 2.7552484027592636\n",
      "Objective after 55 iterations = 6328.6991960745945\n",
      "MSE after 55 iterations = 2.755246022503722\n",
      "Objective after 56 iterations = 6323.341715729331\n",
      "MSE after 56 iterations = 2.7552436913769434\n",
      "Objective after 57 iterations = 6318.136574270832\n",
      "MSE after 57 iterations = 2.7552414082714813\n",
      "Objective after 58 iterations = 6313.0795385038555\n",
      "MSE after 58 iterations = 2.7552391721060907\n",
      "Objective after 59 iterations = 6308.166489240096\n",
      "MSE after 59 iterations = 2.7552369818250906\n",
      "Objective after 60 iterations = 6303.393418398714\n",
      "MSE after 60 iterations = 2.7552348363975234\n",
      "Objective after 61 iterations = 6298.756426171405\n",
      "MSE after 61 iterations = 2.755232734817185\n",
      "Objective after 62 iterations = 6294.251718251424\n",
      "MSE after 62 iterations = 2.755230676101416\n",
      "Objective after 63 iterations = 6289.875603125404\n",
      "MSE after 63 iterations = 2.7552286592909137\n",
      "Objective after 64 iterations = 6285.624489427454\n",
      "MSE after 64 iterations = 2.7552266834489743\n",
      "Objective after 65 iterations = 6281.494883353901\n",
      "MSE after 65 iterations = 2.755224747661118\n",
      "Objective after 66 iterations = 6277.483386137944\n",
      "MSE after 66 iterations = 2.7552228510344263\n",
      "Objective after 67 iterations = 6273.586691583376\n",
      "MSE after 67 iterations = 2.7552209926973195\n",
      "Objective after 68 iterations = 6269.801583656375\n",
      "MSE after 68 iterations = 2.7552191717984322\n",
      "Objective after 69 iterations = 6266.124934133474\n",
      "MSE after 69 iterations = 2.755217387506903\n",
      "Objective after 70 iterations = 6262.553700306501\n",
      "MSE after 70 iterations = 2.755215639011298\n",
      "Objective after 71 iterations = 6259.084922741118\n",
      "MSE after 71 iterations = 2.7552139255195067\n",
      "Objective after 72 iterations = 6255.715723089974\n",
      "MSE after 72 iterations = 2.75521224625794\n",
      "Objective after 73 iterations = 6252.443301958438\n",
      "MSE after 73 iterations = 2.755210600471474\n",
      "Objective after 74 iterations = 6249.264936822002\n",
      "MSE after 74 iterations = 2.755208987422716\n",
      "Objective after 75 iterations = 6246.177979994333\n",
      "MSE after 75 iterations = 2.75520740639168\n",
      "Objective after 76 iterations = 6243.179856645362\n",
      "MSE after 76 iterations = 2.7552058566751696\n",
      "Objective after 77 iterations = 6240.268062867795\n",
      "MSE after 77 iterations = 2.7552043375870325\n",
      "Objective after 78 iterations = 6237.440163791482\n",
      "MSE after 78 iterations = 2.7552028484569147\n",
      "Objective after 79 iterations = 6234.693791744573\n",
      "MSE after 79 iterations = 2.7552013886302187\n",
      "Objective after 80 iterations = 6232.026644460383\n",
      "MSE after 80 iterations = 2.755199957467903\n",
      "Objective after 81 iterations = 6229.4364833290465\n",
      "MSE after 81 iterations = 2.7551985543459394\n",
      "Objective after 82 iterations = 6226.921131692934\n",
      "MSE after 82 iterations = 2.75519717865484\n",
      "Objective after 83 iterations = 6224.478473185301\n",
      "MSE after 83 iterations = 2.7551958297996575\n",
      "Objective after 84 iterations = 6222.106450110579\n",
      "MSE after 84 iterations = 2.7551945071989956\n",
      "Objective after 85 iterations = 6219.803061865588\n",
      "MSE after 85 iterations = 2.7551932102855683\n",
      "Objective after 86 iterations = 6217.566363401439\n",
      "MSE after 86 iterations = 2.755191938505033\n",
      "Objective after 87 iterations = 6215.394463724159\n",
      "MSE after 87 iterations = 2.7551906913160638\n",
      "Objective after 88 iterations = 6213.285524433853\n",
      "MSE after 88 iterations = 2.7551894681902303\n",
      "Objective after 89 iterations = 6211.237758301357\n",
      "MSE after 89 iterations = 2.755188268610983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective after 90 iterations = 6209.249427881521\n",
      "MSE after 90 iterations = 2.755187092074258\n",
      "Objective after 91 iterations = 6207.318844162245\n",
      "MSE after 91 iterations = 2.7551859380875623\n",
      "Objective after 92 iterations = 6205.444365248543\n",
      "MSE after 92 iterations = 2.755184806169715\n",
      "Objective after 93 iterations = 6203.6243950805465\n",
      "MSE after 93 iterations = 2.7551836958510476\n",
      "Objective after 94 iterations = 6201.857382185177\n",
      "MSE after 94 iterations = 2.755182606672543\n",
      "Objective after 95 iterations = 6200.141818460075\n",
      "MSE after 95 iterations = 2.7551815381859104\n",
      "Objective after 96 iterations = 6198.4762379894\n",
      "MSE after 96 iterations = 2.755180489953302\n",
      "Objective after 97 iterations = 6196.859215890833\n",
      "MSE after 97 iterations = 2.7551794615468492\n",
      "Objective after 98 iterations = 6195.289367192376\n",
      "MSE after 98 iterations = 2.7551784525487086\n",
      "Objective after 99 iterations = 6193.765345739354\n",
      "MSE after 99 iterations = 2.7551774625507677\n",
      "Objective after 100 iterations = 6192.28584312967\n",
      "MSE after 100 iterations = 2.7551764911539762\n",
      "Objective after 101 iterations = 6190.849587677592\n",
      "MSE after 101 iterations = 2.755175537968728\n",
      "Objective after 102 iterations = 6189.455343404591\n",
      "MSE after 102 iterations = 2.7551746026143817\n",
      "Objective after 103 iterations = 6188.101909057368\n",
      "MSE after 103 iterations = 2.755173684719025\n",
      "Objective after 104 iterations = 6186.788117151625\n",
      "MSE after 104 iterations = 2.7551727839192757\n",
      "Objective after 105 iterations = 6185.512833041619\n",
      "MSE after 105 iterations = 2.7551718998598536\n",
      "Objective after 106 iterations = 6184.274954014104\n",
      "MSE after 106 iterations = 2.7551710321938527\n",
      "Objective after 107 iterations = 6183.073408406907\n",
      "MSE after 107 iterations = 2.75517018058216\n",
      "Objective after 108 iterations = 6181.907154750918\n",
      "MSE after 108 iterations = 2.7551693446934946\n",
      "Objective after 109 iterations = 6180.7751809349475\n",
      "MSE after 109 iterations = 2.7551685242040462\n",
      "Objective after 110 iterations = 6179.676503393395\n",
      "MSE after 110 iterations = 2.755167718797358\n",
      "Objective after 111 iterations = 6178.610166315193\n",
      "MSE after 111 iterations = 2.755166928164092\n",
      "Objective after 112 iterations = 6177.575240874622\n",
      "MSE after 112 iterations = 2.755166152002062\n",
      "Objective after 113 iterations = 6176.5708244825655\n",
      "MSE after 113 iterations = 2.7551653900158555\n",
      "Objective after 114 iterations = 6175.596040057944\n",
      "MSE after 114 iterations = 2.7551646419165827\n",
      "Objective after 115 iterations = 6174.650035319073\n",
      "MSE after 115 iterations = 2.7551639074220287\n",
      "Objective after 116 iterations = 6173.731982094017\n",
      "MSE after 116 iterations = 2.7551631862563006\n",
      "Objective after 117 iterations = 6172.841075649775\n",
      "MSE after 117 iterations = 2.7551624781497828\n",
      "Objective after 118 iterations = 6171.976534039694\n",
      "MSE after 118 iterations = 2.7551617828386927\n",
      "Objective after 119 iterations = 6171.137597468352\n",
      "MSE after 119 iterations = 2.755161100065251\n",
      "Objective after 120 iterations = 6170.323527674186\n",
      "MSE after 120 iterations = 2.7551604295775673\n",
      "Objective after 121 iterations = 6169.53360732854\n",
      "MSE after 121 iterations = 2.7551597711292026\n",
      "Objective after 122 iterations = 6168.767139451411\n",
      "MSE after 122 iterations = 2.7551591244791824\n",
      "Objective after 123 iterations = 6168.023446842763\n",
      "MSE after 123 iterations = 2.7551584893920227\n",
      "Objective after 124 iterations = 6167.301871529889\n",
      "MSE after 124 iterations = 2.75515786563742\n",
      "Objective after 125 iterations = 6166.601774229598\n",
      "MSE after 125 iterations = 2.7551572529900477\n",
      "Objective after 126 iterations = 6165.922533825072\n",
      "MSE after 126 iterations = 2.7551566512298846\n",
      "Objective after 127 iterations = 6165.263546857381\n",
      "MSE after 127 iterations = 2.7551560601411413\n",
      "Objective after 128 iterations = 6164.624227030519\n",
      "MSE after 128 iterations = 2.755155479513553\n",
      "Objective after 129 iterations = 6164.00400473055\n",
      "MSE after 129 iterations = 2.7551549091409693\n",
      "Objective after 130 iterations = 6163.402326557554\n",
      "MSE after 130 iterations = 2.755154348821793\n",
      "Objective after 131 iterations = 6162.818654870794\n",
      "MSE after 131 iterations = 2.755153798359033\n",
      "Objective after 132 iterations = 6162.252467345997\n",
      "MSE after 132 iterations = 2.7551532575599436\n",
      "Objective after 133 iterations = 6161.703256545451\n",
      "MSE after 133 iterations = 2.755152726235745\n",
      "Objective after 134 iterations = 6161.170529499191\n",
      "MSE after 134 iterations = 2.7551522042021115\n",
      "Objective after 135 iterations = 6160.653807298689\n",
      "MSE after 135 iterations = 2.755151691278474\n",
      "Objective after 136 iterations = 6160.1526247008005\n",
      "MSE after 136 iterations = 2.7551511872884213\n",
      "Objective after 137 iterations = 6159.66652974344\n",
      "MSE after 137 iterations = 2.7551506920588955\n",
      "Objective after 138 iterations = 6159.195083371605\n",
      "MSE after 138 iterations = 2.755150205421179\n",
      "Objective after 139 iterations = 6158.7378590735025\n",
      "MSE after 139 iterations = 2.755149727209751\n",
      "Objective after 140 iterations = 6158.294442527416\n",
      "MSE after 140 iterations = 2.755149257262783\n",
      "Objective after 141 iterations = 6157.864431257568\n",
      "MSE after 141 iterations = 2.755148795421928\n",
      "Objective after 142 iterations = 6157.44743430036\n",
      "MSE after 142 iterations = 2.7551483415321254\n",
      "Objective after 143 iterations = 6157.04307187915\n",
      "MSE after 143 iterations = 2.7551478954418833\n",
      "Objective after 144 iterations = 6156.650975088661\n",
      "MSE after 144 iterations = 2.7551474570026104\n",
      "Objective after 145 iterations = 6156.270785587707\n",
      "MSE after 145 iterations = 2.7551470260691757\n",
      "Objective after 146 iterations = 6155.902155300904\n",
      "MSE after 146 iterations = 2.755146602499173\n",
      "Objective after 147 iterations = 6155.5447461283\n",
      "MSE after 147 iterations = 2.7551461861535755\n",
      "Objective after 148 iterations = 6155.198229663355\n",
      "MSE after 148 iterations = 2.755145776896123\n",
      "Objective after 149 iterations = 6154.862286918776\n",
      "MSE after 149 iterations = 2.755145374593421\n",
      "Objective after 150 iterations = 6154.536608059756\n",
      "MSE after 150 iterations = 2.7551449791148466\n",
      "Objective after 151 iterations = 6154.220892145022\n",
      "MSE after 151 iterations = 2.755144590332686\n",
      "Objective after 152 iterations = 6153.914846874891\n",
      "MSE after 152 iterations = 2.75514420812168\n",
      "Objective after 153 iterations = 6153.618188346351\n",
      "MSE after 153 iterations = 2.755143832359347\n",
      "Objective after 154 iterations = 6153.330640815268\n",
      "MSE after 154 iterations = 2.7551434629257128\n",
      "Objective after 155 iterations = 6153.05193646471\n",
      "MSE after 155 iterations = 2.7551430997032136\n",
      "Objective after 156 iterations = 6152.781815180458\n",
      "MSE after 156 iterations = 2.7551427425769015\n",
      "Objective after 157 iterations = 6152.52002433217\n",
      "MSE after 157 iterations = 2.7551423914339845\n",
      "Objective after 158 iterations = 6152.266318561101\n",
      "MSE after 158 iterations = 2.755142046164266\n",
      "Objective after 159 iterations = 6152.020459573577\n",
      "MSE after 159 iterations = 2.7551417066595243\n",
      "Objective after 160 iterations = 6151.782215940313\n",
      "MSE after 160 iterations = 2.755141372813964\n",
      "Objective after 161 iterations = 6151.551362901409\n",
      "MSE after 161 iterations = 2.7551410445240054\n",
      "Objective after 162 iterations = 6151.327682176633\n",
      "MSE after 162 iterations = 2.755140721688\n",
      "Objective after 163 iterations = 6151.110961781449\n",
      "MSE after 163 iterations = 2.7551404042065246\n",
      "Objective after 164 iterations = 6150.900995847605\n",
      "MSE after 164 iterations = 2.75514009198205\n",
      "Objective after 165 iterations = 6150.697584449393\n",
      "MSE after 165 iterations = 2.7551397849191863\n",
      "Objective after 166 iterations = 6150.500533434361\n",
      "MSE after 166 iterations = 2.7551394829244034\n",
      "Objective after 167 iterations = 6150.30965425899\n",
      "MSE after 167 iterations = 2.7551391859059935\n",
      "Objective after 168 iterations = 6150.124763828995\n",
      "MSE after 168 iterations = 2.7551388937742214\n",
      "Objective after 169 iterations = 6149.945684343844\n",
      "MSE after 169 iterations = 2.7551386064410512\n",
      "Objective after 170 iterations = 6149.7722431462025\n",
      "MSE after 170 iterations = 2.755138323820326\n",
      "Objective after 171 iterations = 6149.604272575092\n",
      "MSE after 171 iterations = 2.7551380458276458\n",
      "Objective after 172 iterations = 6149.441609823448\n",
      "MSE after 172 iterations = 2.755137772379885\n",
      "Objective after 173 iterations = 6149.284096799821\n",
      "MSE after 173 iterations = 2.7551375033964063\n",
      "Objective after 174 iterations = 6149.131579993451\n",
      "MSE after 174 iterations = 2.755137238797174\n",
      "Objective after 175 iterations = 6148.983910343885\n",
      "MSE after 175 iterations = 2.755136978504623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective after 176 iterations = 6148.8409431136315\n",
      "MSE after 176 iterations = 2.7551367224421464\n",
      "Objective after 177 iterations = 6148.702537764905\n",
      "MSE after 177 iterations = 2.755136470534834\n",
      "Objective after 178 iterations = 6148.568557839423\n",
      "MSE after 178 iterations = 2.7551362227094485\n",
      "Objective after 179 iterations = 6148.438870841919\n",
      "MSE after 179 iterations = 2.755135978893863\n",
      "Objective after 180 iterations = 6148.3133481268615\n",
      "MSE after 180 iterations = 2.755135739017546\n",
      "Objective after 181 iterations = 6148.191864788228\n",
      "MSE after 181 iterations = 2.7551355030114055\n",
      "Objective after 182 iterations = 6148.074299552734\n",
      "MSE after 182 iterations = 2.755135270807567\n",
      "Objective after 183 iterations = 6147.960534675639\n",
      "MSE after 183 iterations = 2.7551350423393868\n",
      "Objective after 184 iterations = 6147.850455839888\n",
      "MSE after 184 iterations = 2.755134817541883\n",
      "Objective after 185 iterations = 6147.743952057975\n",
      "MSE after 185 iterations = 2.755134596351097\n",
      "Objective after 186 iterations = 6147.640915576365\n",
      "MSE after 186 iterations = 2.7551343787041427\n",
      "Objective after 187 iterations = 6147.541241783128\n",
      "MSE after 187 iterations = 2.7551341645396503\n",
      "Objective after 188 iterations = 6147.444829117708\n",
      "MSE after 188 iterations = 2.7551339537973165\n",
      "Objective after 189 iterations = 6147.351578983399\n",
      "MSE after 189 iterations = 2.755133746417995\n",
      "Objective after 190 iterations = 6147.261395662441\n",
      "MSE after 190 iterations = 2.755133542343584\n",
      "Objective after 191 iterations = 6147.174186233482\n",
      "MSE after 191 iterations = 2.755133341517252\n",
      "Objective after 192 iterations = 6147.089860491095\n",
      "MSE after 192 iterations = 2.7551331438831084\n",
      "Objective after 193 iterations = 6147.008330868062\n",
      "MSE after 193 iterations = 2.7551329493865566\n",
      "Objective after 194 iterations = 6146.929512359471\n",
      "MSE after 194 iterations = 2.755132757973776\n",
      "Objective after 195 iterations = 6146.853322449079\n",
      "MSE after 195 iterations = 2.755132569592038\n",
      "Objective after 196 iterations = 6146.779681037949\n",
      "MSE after 196 iterations = 2.755132384189648\n",
      "Objective after 197 iterations = 6146.708510374767\n",
      "MSE after 197 iterations = 2.755132201716019\n",
      "Objective after 198 iterations = 6146.6397349885465\n",
      "MSE after 198 iterations = 2.7551320221211615\n",
      "Objective after 199 iterations = 6146.573281622871\n",
      "MSE after 199 iterations = 2.7551318453564018\n",
      "Objective after 200 iterations = 6146.509079172258\n",
      "MSE after 200 iterations = 2.755131671373813\n",
      "Objective after 201 iterations = 6146.447058620157\n",
      "MSE after 201 iterations = 2.7551315001263044\n",
      "Objective after 202 iterations = 6146.387152978971\n",
      "MSE after 202 iterations = 2.7551313315677946\n",
      "Objective after 203 iterations = 6146.329297231423\n",
      "MSE after 203 iterations = 2.7551311656529243\n",
      "Objective after 204 iterations = 6146.273428273903\n",
      "MSE after 204 iterations = 2.7551310023373854\n",
      "Objective after 205 iterations = 6146.219484861289\n",
      "MSE after 205 iterations = 2.755130841577367\n",
      "Objective after 206 iterations = 6146.167407553393\n",
      "MSE after 206 iterations = 2.755130683330162\n",
      "Objective after 207 iterations = 6146.117138662759\n",
      "MSE after 207 iterations = 2.7551305275537663\n",
      "Objective after 208 iterations = 6146.06862220431\n",
      "MSE after 208 iterations = 2.755130374206902\n",
      "Objective after 209 iterations = 6146.021803846011\n",
      "MSE after 209 iterations = 2.7551302232490915\n",
      "Objective after 210 iterations = 6145.976630861305\n",
      "MSE after 210 iterations = 2.7551300746404754\n",
      "Objective after 211 iterations = 6145.933052082616\n",
      "MSE after 211 iterations = 2.7551299283421486\n",
      "Objective after 212 iterations = 6145.891017856326\n",
      "MSE after 212 iterations = 2.755129784315841\n",
      "Objective after 213 iterations = 6145.8504799991515\n",
      "MSE after 213 iterations = 2.755129642523826\n",
      "Objective after 214 iterations = 6145.811391755352\n",
      "MSE after 214 iterations = 2.7551295029290985\n",
      "Objective after 215 iterations = 6145.7737077557595\n",
      "MSE after 215 iterations = 2.7551293654956623\n",
      "Objective after 216 iterations = 6145.737383977352\n",
      "MSE after 216 iterations = 2.755129230187613\n",
      "Objective after 217 iterations = 6145.702377704497\n",
      "MSE after 217 iterations = 2.755129096970159\n",
      "Objective after 218 iterations = 6145.668647491019\n",
      "MSE after 218 iterations = 2.755128965809017\n",
      "Objective after 219 iterations = 6145.636153123472\n",
      "MSE after 219 iterations = 2.7551288366702624\n",
      "Objective after 220 iterations = 6145.604855585256\n",
      "MSE after 220 iterations = 2.755128709520964\n",
      "Objective after 221 iterations = 6145.574717022079\n",
      "MSE after 221 iterations = 2.7551285843285407\n",
      "Objective after 222 iterations = 6145.545700708176\n",
      "MSE after 222 iterations = 2.755128461061073\n",
      "Objective after 223 iterations = 6145.5177710135595\n",
      "MSE after 223 iterations = 2.7551283396871216\n",
      "Objective after 224 iterations = 6145.490893372275\n",
      "MSE after 224 iterations = 2.7551282201759677\n",
      "Objective after 225 iterations = 6145.465034251389\n",
      "MSE after 225 iterations = 2.7551281024972116\n",
      "Objective after 226 iterations = 6145.440161121001\n",
      "MSE after 226 iterations = 2.755127986621249\n",
      "Objective after 227 iterations = 6145.416242425273\n",
      "MSE after 227 iterations = 2.755127872518889\n",
      "Objective after 228 iterations = 6145.393247553764\n",
      "MSE after 228 iterations = 2.75512776016132\n",
      "Objective after 229 iterations = 6145.371146814237\n",
      "MSE after 229 iterations = 2.7551276495204515\n",
      "Objective after 230 iterations = 6145.349911405779\n",
      "MSE after 230 iterations = 2.7551275405685307\n",
      "Objective after 231 iterations = 6145.329513392863\n",
      "MSE after 231 iterations = 2.755127433278451\n",
      "Objective after 232 iterations = 6145.309925680204\n",
      "MSE after 232 iterations = 2.755127327623333\n",
      "Objective after 233 iterations = 6145.291121988144\n",
      "MSE after 233 iterations = 2.7551272235770137\n",
      "Objective after 234 iterations = 6145.273076829062\n",
      "MSE after 234 iterations = 2.7551271211136816\n",
      "Objective after 235 iterations = 6145.255765484036\n",
      "MSE after 235 iterations = 2.755127020207931\n",
      "Objective after 236 iterations = 6145.2391639806665\n",
      "MSE after 236 iterations = 2.755126920834905\n",
      "Objective after 237 iterations = 6145.223249071015\n",
      "MSE after 237 iterations = 2.7551268229700527\n",
      "Objective after 238 iterations = 6145.207998210588\n",
      "MSE after 238 iterations = 2.755126726589404\n",
      "Objective after 239 iterations = 6145.193389537847\n",
      "MSE after 239 iterations = 2.7551266316692247\n",
      "Objective after 240 iterations = 6145.179401854016\n",
      "MSE after 240 iterations = 2.755126538186401\n",
      "Objective after 241 iterations = 6145.166014604002\n",
      "MSE after 241 iterations = 2.755126446117895\n",
      "Objective after 242 iterations = 6145.153207857072\n",
      "MSE after 242 iterations = 2.7551263554413787\n",
      "Objective after 243 iterations = 6145.140962289155\n",
      "MSE after 243 iterations = 2.7551262661347695\n",
      "Objective after 244 iterations = 6145.129259164617\n",
      "MSE after 244 iterations = 2.755126178176372\n",
      "Objective after 245 iterations = 6145.118080319207\n",
      "MSE after 245 iterations = 2.7551260915449074\n",
      "Objective after 246 iterations = 6145.107408143204\n",
      "MSE after 246 iterations = 2.7551260062193665\n",
      "Objective after 247 iterations = 6145.09722556525\n",
      "MSE after 247 iterations = 2.7551259221791637\n",
      "Objective after 248 iterations = 6145.087516036459\n",
      "MSE after 248 iterations = 2.755125839404119\n"
     ]
    }
   ],
   "source": [
    "while iterations < 10 or objective - newObjective > 0.01:\n",
    "    mse, objective = newMSE, newObjective\n",
    "    newMSE, newObjective = iterate(1)\n",
    "    iterations += 1\n",
    "    print(\"Objective after \"\n",
    "        + str(iterations) + \" iterations = \" + str(newObjective))\n",
    "    print(\"MSE after \"\n",
    "        + str(iterations) + \" iterations = \" + str(newMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4544f0f-39ac-4452-9180-baa378507201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE = 3.3620657269506733\n"
     ]
    }
   ],
   "source": [
    "validMSE = 0\n",
    "for u,g,d in hoursValid:\n",
    "    r = d['hours_transformed']\n",
    "    bu = 0\n",
    "    bi = 0\n",
    "    if u in betaU:\n",
    "        bu = betaU[u]\n",
    "    if g in betaI:\n",
    "        bi = betaI[g]\n",
    "    prediction = alpha + bu + bi\n",
    "    validMSE += (r - prediction)**2\n",
    "\n",
    "validMSE /= len(hoursValid)\n",
    "print(\"Validation MSE = \" + str(validMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6534a08d-013e-4353-a12c-b1f2bbed5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = validMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc0e3695-682b-4d65-9576-c59795d04930",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9d419e4-e8c4-4766-b189-d77fbe608417",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a48cc70-1c2c-40df-9843-fea1f287a10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum betaU = u60898505 (5.828316739259239)\n",
      "Maximum betaI = g17604638 (5.495973739724736)\n",
      "Minimum betaU = u13037838 (-3.0057870148761894)\n",
      "Minimum betaI = g84397720 (-2.809328679823356)\n"
     ]
    }
   ],
   "source": [
    "betaUs = [(betaU[u], u) for u in betaU]\n",
    "betaIs = [(betaI[i], i) for i in betaI]\n",
    "betaUs.sort()\n",
    "betaIs.sort()\n",
    "\n",
    "print(\"Maximum betaU = \" + str(betaUs[-1][1]) + ' (' + str(betaUs[-1][0]) + ')')\n",
    "print(\"Maximum betaI = \" + str(betaIs[-1][1]) + ' (' + str(betaIs[-1][0]) + ')')\n",
    "print(\"Minimum betaU = \" + str(betaUs[0][1]) + ' (' + str(betaUs[0][0]) + ')')\n",
    "print(\"Minimum betaI = \" + str(betaIs[0][1]) + ' (' + str(betaIs[0][0]) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65b17529-ade3-4cdf-a5c1-b17b06e68237",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = [betaUs[-1][0], betaUs[0][0], betaIs[-1][0], betaIs[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4eeaf180-3bd8-4acb-aef5-86b044521e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.828316739259239, -3.0057870148761894, 5.495973739724736, -2.809328679823356]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c9faa5c-2bc1-4d51-ae29-df2d82c9372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q7'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c604fd19-2fb8-44bf-82b5-33797f534707",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30b8cbba-d0ec-46a6-b079-1c9a0e188971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective after 2 iterations = 23723.40581939076\n",
      "MSE after 2 iterations = 2.7788624145856393\n",
      "Objective after 3 iterations = 23510.585432916247\n",
      "MSE after 3 iterations = 2.77950918417825\n",
      "Objective after 4 iterations = 23487.108448891875\n",
      "MSE after 4 iterations = 2.779632986564118\n",
      "Objective after 5 iterations = 23482.603926859705\n",
      "MSE after 5 iterations = 2.7796579991997605\n",
      "Objective after 6 iterations = 23481.074962137227\n",
      "MSE after 6 iterations = 2.7796657598050394\n",
      "Objective after 7 iterations = 23480.130273496496\n",
      "MSE after 7 iterations = 2.7796701526871908\n",
      "Objective after 8 iterations = 23479.338493177987\n",
      "MSE after 8 iterations = 2.7796737556337834\n",
      "Objective after 9 iterations = 23478.61426569989\n",
      "MSE after 9 iterations = 2.7796770737497773\n",
      "Objective after 10 iterations = 23477.938819973006\n",
      "MSE after 10 iterations = 2.7796802099553215\n",
      "Objective after 11 iterations = 23477.30667015269\n",
      "MSE after 11 iterations = 2.7796831871553858\n",
      "Objective after 12 iterations = 23476.715002575234\n",
      "MSE after 12 iterations = 2.7796860126194227\n",
      "Objective after 13 iterations = 23476.16153911946\n",
      "MSE after 13 iterations = 2.7796886909931113\n",
      "Objective after 14 iterations = 23475.64412291767\n",
      "MSE after 14 iterations = 2.7796912268421847\n",
      "Objective after 15 iterations = 23475.16065379043\n",
      "MSE after 15 iterations = 2.7796936250542577\n",
      "Objective after 16 iterations = 23474.709090840137\n",
      "MSE after 16 iterations = 2.7796958908157596\n",
      "Objective after 17 iterations = 23474.28746437589\n",
      "MSE after 17 iterations = 2.779698029519529\n",
      "Objective after 18 iterations = 23473.89388602729\n",
      "MSE after 18 iterations = 2.779700046673986\n",
      "Objective after 19 iterations = 23473.52655552462\n",
      "MSE after 19 iterations = 2.779701947827451\n",
      "Objective after 20 iterations = 23473.183764494268\n",
      "MSE after 20 iterations = 2.7797037385059995\n",
      "Objective after 21 iterations = 23472.863897891206\n",
      "MSE after 21 iterations = 2.7797054241651318\n",
      "Objective after 22 iterations = 23472.56543363089\n",
      "MSE after 22 iterations = 2.7797070101510615\n",
      "Objective after 23 iterations = 23472.28694088742\n",
      "MSE after 23 iterations = 2.779708501672005\n",
      "Objective after 24 iterations = 23472.027077431016\n",
      "MSE after 24 iterations = 2.779709903775975\n",
      "Objective after 25 iterations = 23471.784586296137\n",
      "MSE after 25 iterations = 2.779711221335141\n",
      "Objective after 26 iterations = 23471.558292009235\n",
      "MSE after 26 iterations = 2.7797124590347004\n",
      "Objective after 27 iterations = 23471.347096550493\n",
      "MSE after 27 iterations = 2.779713621366386\n",
      "Objective after 28 iterations = 23471.149975187873\n",
      "MSE after 28 iterations = 2.7797147126247004\n",
      "Objective after 29 iterations = 23470.965972280068\n",
      "MSE after 29 iterations = 2.779715736906046\n",
      "Objective after 30 iterations = 23470.794197126375\n",
      "MSE after 30 iterations = 2.7797166981101076\n",
      "Objective after 31 iterations = 23470.633819918305\n",
      "MSE after 31 iterations = 2.7797175999427637\n",
      "Objective after 32 iterations = 23470.48406782843\n",
      "MSE after 32 iterations = 2.779718445919436\n",
      "Objective after 33 iterations = 23470.344221265397\n",
      "MSE after 33 iterations = 2.7797192393715595\n",
      "Objective after 34 iterations = 23470.21361030992\n",
      "MSE after 34 iterations = 2.779719983451391\n",
      "Objective after 35 iterations = 23470.09161133967\n",
      "MSE after 35 iterations = 2.7797206811388713\n",
      "Objective after 36 iterations = 23469.977643848022\n",
      "MSE after 36 iterations = 2.779721335248263\n",
      "Objective after 37 iterations = 23469.87116745546\n",
      "MSE after 37 iterations = 2.779721948434876\n",
      "Objective after 38 iterations = 23469.77167910807\n",
      "MSE after 38 iterations = 2.779722523202685\n",
      "Objective after 39 iterations = 23469.678710460365\n",
      "MSE after 39 iterations = 2.7797230619107585\n",
      "Objective after 40 iterations = 23469.59182543196\n",
      "MSE after 40 iterations = 2.7797235667807447\n",
      "Objective after 41 iterations = 23469.510617931217\n",
      "MSE after 41 iterations = 2.779724039903443\n",
      "Objective after 42 iterations = 23469.434709737787\n",
      "MSE after 42 iterations = 2.779724483245663\n",
      "Objective after 43 iterations = 23469.363748532498\n",
      "MSE after 43 iterations = 2.779724898656832\n",
      "Objective after 44 iterations = 23469.297406068945\n",
      "MSE after 44 iterations = 2.7797252878753573\n",
      "Objective after 45 iterations = 23469.23537647397\n",
      "MSE after 45 iterations = 2.7797256525344936\n",
      "Objective after 46 iterations = 23469.17737467167\n",
      "MSE after 46 iterations = 2.779725994168456\n",
      "Objective after 47 iterations = 23469.12313492053\n",
      "MSE after 47 iterations = 2.7797263142178\n",
      "Objective after 48 iterations = 23469.072409456763\n",
      "MSE after 48 iterations = 2.7797266140350025\n",
      "Objective after 49 iterations = 23469.024967235713\n",
      "MSE after 49 iterations = 2.7797268948894795\n",
      "Objective after 50 iterations = 23468.98059276422\n",
      "MSE after 50 iterations = 2.7797271579723106\n",
      "Objective after 51 iterations = 23468.93908501895\n",
      "MSE after 51 iterations = 2.779727404400979\n",
      "Objective after 52 iterations = 23468.900256440287\n",
      "MSE after 52 iterations = 2.7797276352238685\n",
      "Objective after 53 iterations = 23468.863932002438\n",
      "MSE after 53 iterations = 2.7797278514239063\n",
      "Objective after 54 iterations = 23468.829948347\n",
      "MSE after 54 iterations = 2.779728053923274\n",
      "Objective after 55 iterations = 23468.79815298184\n",
      "MSE after 55 iterations = 2.779728243586385\n",
      "Objective after 56 iterations = 23468.768403535105\n",
      "MSE after 56 iterations = 2.7797284212235973\n",
      "Objective after 57 iterations = 23468.74056706396\n",
      "MSE after 57 iterations = 2.7797285875948345\n",
      "Objective after 58 iterations = 23468.714519412064\n",
      "MSE after 58 iterations = 2.779728743412086\n",
      "Objective after 59 iterations = 23468.69014461272\n",
      "MSE after 59 iterations = 2.7797288893428655\n",
      "Objective after 60 iterations = 23468.66733433392\n",
      "MSE after 60 iterations = 2.7797290260127636\n",
      "Objective after 61 iterations = 23468.645987362248\n",
      "MSE after 61 iterations = 2.7797291540079954\n",
      "Objective after 62 iterations = 23468.62600912509\n",
      "MSE after 62 iterations = 2.7797292738778396\n",
      "Objective after 63 iterations = 23468.60731124295\n",
      "MSE after 63 iterations = 2.7797293861370367\n",
      "Objective after 64 iterations = 23468.589811115202\n",
      "MSE after 64 iterations = 2.779729491267721\n",
      "Objective after 65 iterations = 23468.573431534234\n",
      "MSE after 65 iterations = 2.779729589721945\n",
      "Objective after 66 iterations = 23468.55810032585\n",
      "MSE after 66 iterations = 2.7797296819228747\n",
      "Objective after 67 iterations = 23468.54375001454\n",
      "MSE after 67 iterations = 2.779729768267079\n",
      "Objective after 68 iterations = 23468.53031751197\n",
      "MSE after 68 iterations = 2.7797298491260793\n",
      "Objective after 69 iterations = 23468.517743826334\n",
      "MSE after 69 iterations = 2.7797299248479392\n",
      "Objective after 70 iterations = 23468.50597379238\n",
      "MSE after 70 iterations = 2.779729995758566\n",
      "Objective after 71 iterations = 23468.494955818587\n",
      "MSE after 71 iterations = 2.7797300621633627\n",
      "Objective after 72 iterations = 23468.484641652325\n",
      "MSE after 72 iterations = 2.7797301243483794\n",
      "Objective after 73 iterations = 23468.474986160585\n",
      "MSE after 73 iterations = 2.77973018258143\n"
     ]
    }
   ],
   "source": [
    "# Better lambda...\n",
    "\n",
    "iterations = 1\n",
    "while iterations < 10 or objective - newObjective > 0.01:\n",
    "    mse, objective = newMSE, newObjective\n",
    "    newMSE, newObjective = iterate(5)\n",
    "    iterations += 1\n",
    "    print(\"Objective after \" + str(iterations) + \" iterations = \" + str(newObjective))\n",
    "    print(\"MSE after \" + str(iterations) + \" iterations = \" + str(newMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1aa5dbce-40a7-429f-a499-652845543d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa01029d-a130-4389-9f0c-bf18fb3726f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE = 3.3246506094357864\n"
     ]
    }
   ],
   "source": [
    "validMSE = 0\n",
    "for u,g,d in hoursValid:\n",
    "    r = d['hours_transformed']\n",
    "    bu = 0\n",
    "    bi = 0\n",
    "    if u in betaU:\n",
    "        bu = betaU[u]\n",
    "    if g in betaI:\n",
    "        bi = betaI[g]\n",
    "    prediction = alpha + bu + bi\n",
    "    validMSE += (r - prediction)**2\n",
    "\n",
    "validMSE /= len(hoursValid)\n",
    "print(\"Validation MSE = \" + str(validMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b95c8e49-d120-4367-a20f-a39381776979",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q8'] = (5.0, validMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe2dcb96-86a0-473e-980b-340435715ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q8'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "90a7cd55-1f58-42a5-8c35-4debf80a3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Hours_tf.csv\", 'w')\n",
    "for l in open(\"/Users/zhiqiaogong/Projects/JupyterNotebook/cse258/hw3/pairs_Hours.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    # bu = 0\n",
    "    # bi = 0\n",
    "    # if u in betaU:\n",
    "    #     bu = betaU[u]\n",
    "    # if g in betaI:\n",
    "    #     bi = betaI[g]\n",
    "    p = modelLFM.predict(userIDs[u], itemIDs[g]).numpy()\n",
    "    _ = predictions.write(u + ',' + g + ',' + str(p) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5fe92e3-3ab1-4858-858c-eeb732d964f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw3.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc82717-0ee6-4d72-a27f-c7aa377efb18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'implicit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimplicit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bpr\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVD, Reader, Dataset\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'implicit'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import random\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from implicit import bpr\n",
    "from surprise import SVD, Reader, Dataset\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9676dc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-macos==2.15.0 (from tensorflow)\n",
      "  Using cached tensorflow_macos-2.15.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached h5py-3.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.0)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached wrapt-1.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached grpcio-1.59.3-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached wheel-0.41.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached tensorflow-2.15.0-cp311-cp311-macosx_12_0_arm64.whl (2.1 kB)\n",
      "Using cached tensorflow_macos-2.15.0-cp311-cp311-macosx_12_0_arm64.whl (208.8 MB)\n",
      "Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached grpcio-1.59.3-cp311-cp311-macosx_10_10_universal2.whl (9.6 MB)\n",
      "Using cached h5py-3.10.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Downloading libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl (20.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl (1.2 MB)\n",
      "Using cached tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "Using cached protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-macosx_12_0_arm64.whl (1.9 MB)\n",
      "Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Using cached wrapt-1.14.1-cp311-cp311-macosx_11_0_arm64.whl (36 kB)\n",
      "Using cached google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "Using cached google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Using cached wheel-0.41.3-py3-none-any.whl (65 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, werkzeug, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, astunparse, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.23.4 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.34.0 tensorflow-macos-2.15.0 termcolor-2.3.0 typing-extensions-4.8.0 werkzeug-3.0.1 wheel-0.41.3 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10572107-2e0a-46d8-bc51-30e8fe2728f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Downloading implicit-0.7.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from implicit) (1.26.0)\n",
      "Requirement already satisfied: scipy>=0.16 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from implicit) (1.11.3)\n",
      "Collecting tqdm>=4.27 (from implicit)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from implicit) (3.2.0)\n",
      "Downloading implicit-0.7.2-cp311-cp311-macosx_11_0_arm64.whl (761 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.6/761.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, implicit\n",
      "Successfully installed implicit-0.7.2 tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d55cf9c-eae6-496d-9fb4-68762cec2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting surprise\n",
      "  Using cached surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
      "Collecting scikit-surprise (from surprise)\n",
      "  Using cached scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[59 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m <string>:65: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001b[31m   \u001b[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m /Library/Frameworks/Python.framework/Versions/3.11/bin/python3.11: No module named pip\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/setuptools/installer.py\", line 101, in _fetch_build_egg_no_warn\n",
      "  \u001b[31m   \u001b[0m     subprocess.check_call(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py\", line 413, in check_call\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, cmd)\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['/Library/Frameworks/Python.framework/Versions/3.11/bin/python3.11', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/tmpvgs2qz86', '--quiet', 'numpy>=1.17.3']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 355, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 325, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 507, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super(_BuildMetaLegacyBackend, self).run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 341, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 65, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 662, in fetch_build_eggs\n",
      "  \u001b[31m   \u001b[0m     return _fetch_build_eggs(self, requires)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/setuptools/installer.py\", line 38, in _fetch_build_eggs\n",
      "  \u001b[31m   \u001b[0m     resolved_dists = pkg_resources.working_set.resolve(\n",
      "  \u001b[31m   \u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/pkg_resources/__init__.py\", line 829, in resolve\n",
      "  \u001b[31m   \u001b[0m     dist = self._resolve_dist(\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/pkg_resources/__init__.py\", line 865, in _resolve_dist\n",
      "  \u001b[31m   \u001b[0m     dist = best[req.key] = env.best_match(\n",
      "  \u001b[31m   \u001b[0m                            ^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/pkg_resources/__init__.py\", line 1135, in best_match\n",
      "  \u001b[31m   \u001b[0m     return self.obtain(req, installer)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/pkg_resources/__init__.py\", line 1147, in obtain\n",
      "  \u001b[31m   \u001b[0m     return installer(requirement)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/pip-build-env-b6zgntks/overlay/lib/python3.11/site-packages/setuptools/installer.py\", line 103, in _fetch_build_egg_no_warn\n",
      "  \u001b[31m   \u001b[0m     raise DistutilsError(str(e)) from e\n",
      "  \u001b[31m   \u001b[0m distutils.errors.DistutilsError: Command '['/Library/Frameworks/Python.framework/Versions/3.11/bin/python3.11', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/wz/vhzwv2fs6vqg6zknn8vqfsk80000gn/T/tmpvgs2qz86', '--quiet', 'numpy>=1.17.3']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --use-pep517 surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ac2d33-7067-481d-976b-881b00069854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.41.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (65.5.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (23.3.1)\n",
      "Using cached setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 65.5.0\n",
      "    Uninstalling setuptools-65.5.0:\n",
      "      Successfully uninstalled setuptools-65.5.0\n",
      "Successfully installed setuptools-68.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wheel setuptools pip --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba07197-de7f-48d5-b4ca-e94cc9cbabdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
